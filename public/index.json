[
{
	"uri": "/introduction-to-r/",
	"title": "Introduction to R",
	"tags": [],
	"description": "",
	"content": " Introduction to R Setting up your working environment in R studio\n"
},
{
	"uri": "/introduction-to-r/01_r-studio-and-the-coding-environment/",
	"title": "R Studio &amp; Coding Environment",
	"tags": [],
	"description": "",
	"content": "Introduction\rThis module will provide an introduction into the R statisitical environment, going through the basics of data analysis and graphing for publication quality results.\rBy the end of this module, you should be able to:\n\rUnderstand and use the R studio working environment\rImport and manipulate data files\rUndertake linear (ANOVA, regression) and generalised linear (logistic regression) models and associated assumptions/comparisons\rUndertake basic multivariate techniques (PCA, MDS)\rConstruct bar plots and scatterplots in ggplot\r\rWhat is R?\rR is a language and environment for statistical computing and graphics. R is free/open source software and as a result, has a community of dedicated statisticans, coders and developers increasing the capabilities and usability of the platform.\rR primarily runs as a command-line program.\nThis is a big entry barrier to many starting to learn R, so most people have turned to “R Studio”.\nBase R\n\r\rWhy R?\rSince R is free and open source, it is a program and skill that can be carried with you across many institutions and jobs and is for many, the single solution for statistical analysis, graphing and even GIS/spatial analysis. Programs like JMP, SPSS and ARCGIS cost 100s if not 1000s of dollars and are quickly outdated by new versions.\nHowever, the biggest uses of R come from its sharability and openness. Collaborating and sharing data analysis with R requires only the script and raw data. All data manipulations are done within R, requiring no editing or manipulating of your raw excel data.\n\rR Studio\rThe R Studio Environment\n\rR studio “reskins” the standard R environment, giving space for script writing, help, graphics output and tracking of data files. Due to the ease of working in R studio, thats what we will be using. R studio can provide an array of functions from statistical analysis and graphing, GIS/spatial analysis, presentations, document preparation (all of these tutorials are written in R) and even novel functions like interacive graphs and tweeting.\n\rLayout\rR studio is separated into 4 panels:\n\rThe top-left panel (blue) is the editor (or script window) where you can view and write your R script. This is a saveable document of code. Running code is as simple as Ctrl+Enter on a line of code or pressing the run button in the top-right of this window.\n\rThe bottom-left (red) is the console. This is the standard R environment where you can run code directly, or view the output of your script as you run it.\n\rThe top-right (green) is your workspace. This lists each “object” as you create them through your analyses. Clicking a data-frame object will allow you to view it.\n\rThe bottom-left (black) has lists of files and packages as well as the help window (quickly access by typing ? before any command) and plots which shows any graphical output.\n\r\rNow we have an idea of what R is, it is time to install R \u0026amp; R Studio onto your computer.\nInstall Instructions\n1. Click Here to visit the R webpage and select one of the Australian mirrors to download (CSIRO, University of Melbourne etc.)\n2. Select your version (Windows or Mac), then download the base subdirectory\n3. Once installed, visit R studio to download R studio desktop.\n Once both of those are installed, you can now proceed to open up Rstudio\n\r\r"
},
{
	"uri": "/data-exploration-and-manipulation/",
	"title": "Data Exploration and Manipulation",
	"tags": [],
	"description": "",
	"content": " Data Exploration and Manipulation Exploring data in its native format and manipulating it using Tidyverse\n"
},
{
	"uri": "/data-exploration-and-manipulation/1_packages/",
	"title": "Packages",
	"tags": [],
	"description": "",
	"content": "Now that you have setup your R environment and read in your first data set, we can begin to modify and add to our data as necessary.\nNow for the majority of this module, we will be working with a package called Tidyverse. Packages are collections of data, R functions and complied code to add extra features outside of the general base R environment. Packages are central to expanding the possibilities of R. The ability to do advanced graphing, GIS, complicated analyses, multivariate analyses etc. are all due to contributed packages.\nTidyverse is a unique case as it is a collection of R packages that all use similar coding syntax\nTo install a package into R there are two options:\rOption 1 is to select the packages tab in the help/viewer window \u0026amp; click the install button.\nThen type the package name in the packages box (note: ensure that it is installing from Repository/CRAN)\nOption 2 is to use the following code, replacing the “tidyverse” with the package of your choice. This should be used in the console, rather than the script window as you only need to install the package once.\ninstall.packages(\u0026quot;tidyverse\u0026quot;)\rOnce you have installed tidyverse, simply load it into your current workspace with the following command (in your script)\nlibrary(tidyverse)\rIn general, it is good practice to place the library() commands for your whole document at the top before anything else. This allows people reading your code to load in any packages they will need at the beginning before anything else.\n For example, here is the first few lines of one of my own scripts:\nThis shows the reader of my code what packages need to be installed to run my analysis. I\ralso write notes to myself to remind myself what specific packages are for. Once you start accumulating packages, its hard to remember what each one does. I find this particularly useful for packages I only use for a particular function (such as the agricolae package which I use for tukeys letter reports).\n\r"
},
{
	"uri": "/introduction-to-r/02_setting-up-your-workspace/",
	"title": "Setting up your workspace",
	"tags": [],
	"description": "",
	"content": "There are very quick ways to open R and begin coding, however, having an organised, well-structured working directory in your computer can save you hours of hassle and make your code much easier to share. As biology and data science are becoming increasingly complex many are turning to computer intensive, coding based software (like you!). With this movement in data science and open access, having our code reproducible, transparent and understandable is key. So why not start off like that.\nThe first part of this tutorial follows many practices outlined in the fantastic “Guide to Reproducible Code in Ecology and Evolution” from the British Ecological Society. I strongly recommend reading this at some stage.\n File system\rBefore we jump into R, we are going to create a clean and managable folder system.\rCreate a new folder in a location of your choosing (e.g. My Documents or Desktop) called R-tutorials\n This can be named anything you like, but try to keep it relevant and understandable (for future you).\nIn this new folder, create a series of new folders called:\n- data\n- doc\n- figs\n- output\n- R\n Here is the basic outline for these folders:\n\r\rThe data folder if where you store your raw/input data\r\r\rThe doc folder is where you store the manuscript for the project\r\r\rThe figs folder is where all of your figures will be stored from the analyses\r\r\rThe output folder is where you keep any intermediate datasets generated by your analysis, result reports etc.\r\r\rThe R folder is pretty self-explanatory but it is where we will store all of our R scripts, notebooks etc.\r\r\rNow, let’s move on to the next step.\n\r"
},
{
	"uri": "/introduction-to-r/03_creating-a-project/",
	"title": "Creating a project",
	"tags": [],
	"description": "",
	"content": "Now that we have our folder setup, lets move into R studio and create our project.\nIn R, a project file stores your current Rstudio working environment in a file within your file system. This means, if you finish your work for the day halfway through an analysis, you can open up your project file the next day and continue straight from where you left off. The other major benefit is that a project file sets your default working directory. This means, when you need to open a file, you only need to do so from the place of your project file. This will make sense in a moment.\nCreating your project\rTo create a project, click file \u0026gt; new project and select existing directory\n Then, browse to your new file system we just created and save the project into the base/root of that directory.\n The real advantage to using project files is simplifying file opening and saving. We will cover this in more detail in the next page, but essentially, when reading data into R we normally have to specify the entire filepath\nsurveydata \u0026lt;- read.csv(\u0026quot;C:/Users/Mitch/Documents/surveydata.csv\u0026quot;)\ror, set a working directory\nsetwd(\u0026quot;C:/Users/Mitch/Documents\u0026quot;)\rsurveydata \u0026lt;- read.csv(\u0026quot;surveydata.csv\u0026quot;)\rThe creation of a project cuts out this step by setting our working directory. This means, anytime we want to open a file, we just need to specify the folders within our project directory. So using our new filing system, we would just need to specify the following:\nsurveydata \u0026lt;- read.csv(\u0026quot;Data/surveydata.csv\u0026quot;)\rIf you have multiple projects, or want to close the current one, simply click the project name in the top right of R studio\n\r"
},
{
	"uri": "/introduction-to-r/4_notebooks/",
	"title": "Notebooks and Markdown",
	"tags": [],
	"description": "",
	"content": "The first step when opening a new R studio environment or project is creating a script or notebook for working in. Scripts are basic text files where all code is executable. Writing non-code in a script requires the use of #’s (which can look messy and confusing) like so:\nread.csv(\u0026quot;datafile.csv\u0026quot;) # this code reads a csv (data) file into R. The command read.csv requires brackets with the filepath to the file in quotations. # in this code, none of the #\u0026#39;s will run. so if I # the read.csv command, it will not run. like so:\r# read.csv(\u0026quot;datafile.csv\u0026quot;)\rA whole document of the above example can get messy and hard to understand.\nIn a notebook, we separate normal text from code by inserting “code chunks” (insert \u0026gt; R in the top right of the window). Chunks are specialised areas in the notebook for code only.\rChunks separate code from text, making it easier to write notes and read. These tutorials have been written in a notebook.\n To create a notebook or script, simply use the pulldown menus file \u0026gt; new file and select either a script, markdown or notebook one. Then save the document by hitting the disk icon (or file \u0026gt; save) R studio will prompt you to install some packages to use a notebook. Do so and then read the text in the notebook. Once you understand, then clear everything below the “output:” — area. As stated above, click the insert pulldown menu in the script window and click R to insert a code chunk. All code in a notebook must be written in a chunk   IMPORTANT!\r\rWhen working in a project, wherever you save your project will become the default “directory”. R will look here for files first. If you want to set your working directory elsewhere, use the below code.\r\r# Only do have not created a project\rsetwd(\u0026quot;Drive:/Folder1/Folder2\u0026quot;)\r# insert your folders path in the brackets\r# this will tell R to look here for files and \u0026quot;generally\u0026quot; save things here as well.\r# e.g. C:/Users/Mitch/Documents/R/\rAfter writing your code, you can click run, run selected line(s), run current chunk or press Ctrl + Enter on the line your cursor is on\n Get used to this, you will do this ALOT\n\r"
},
{
	"uri": "/data-exploration-and-manipulation/2_viewingdata/",
	"title": "Viewing your data",
	"tags": [],
	"description": "",
	"content": "Now we have become acquainted with our working directories and the R environment, its time to explore our newely imported data. For this, we will be using the weeds dataset.\rEnsure your data is loaded in and then either use the View() command:\nweeds \u0026lt;- read.csv(\u0026quot;weeds.csv\u0026quot;)\rView(weeds)\r# This will open up a new tab to view your data\ror click the variable name in the environment window.\nThis should bring up a separate tab in Rstudio which you should be able to see the 4 columns (weeds, soil, species \u0026amp; flowers.m3).\nNow we can see our data, we can investigate the way R has input our data. The best thing to do is to ensure your categorical variables are categorical, and our continuous are continuous, much like we do in programs like JMP.\nIn JMP, we have the icons to identify categorical/nominal, ordinal or continuous. In R, all we do is run a single line of code to view the same thing across the different columns.\nstr(weeds)\r## \u0026#39;data.frame\u0026#39;: 48 obs. of 4 variables:\r## $ weeds : Factor w/ 2 levels \u0026quot;native\u0026quot;,\u0026quot;weed\u0026quot;: 2 2 2 2 2 2 2 2 2 2 ...\r## $ soil : Factor w/ 2 levels \u0026quot;sandstone\u0026quot;,\u0026quot;shale\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ...\r## $ species : Factor w/ 3 levels \u0026quot;Coprosma\u0026quot;,\u0026quot;Olearia\u0026quot;,..: 1 1 1 1 3 3 3 3 2 2 ...\r## $ flowers.m3: int 14 17 23 26 35 45 36 28 28 39 ...\r# str stands for \u0026quot;structure\u0026quot; and will tell us the formats of each data column, as well as the number of levels when we have a factor (categorical) column\rstr() also shows us the number of levels we have in a factor. So if we put in a bad dataset with different capitalisations or misspellings on factor levels, we can identify here how many we want vs. how many we have. Its a quick and easy way to assess your data.\nAs you can see in the weeds example, we have weeds, soil \u0026amp; species as factors (categorical) and flowers.m3 as an integer (one of many continuous data types, in this case, whole numbers).\nWe will follow up on how to fix an incorrect column shortly\nOther data viewing commands can be used to view certain aspects of your data without bringing up the entire data set in a new tab. These are as follows:\nhead(weeds) # This will show the top few rows of your data so you can check it without loading the entire table\r## weeds soil species flowers.m3\r## 1 weed sandstone Coprosma 14\r## 2 weed sandstone Coprosma 17\r## 3 weed sandstone Coprosma 23\r## 4 weed sandstone Coprosma 26\r## 5 weed sandstone Pultenaea 35\r## 6 weed sandstone Pultenaea 45\rtail(weeds) # The same as head() but shows the bottom rows\r## weeds soil species flowers.m3\r## 43 native shale Pultenaea 49\r## 44 native shale Pultenaea 20\r## 45 native shale Olearia 32\r## 46 native shale Olearia 51\r## 47 native shale Olearia 47\r## 48 native shale Olearia 55\rdim(weeds) # This gives you the number of rows and columns\r## [1] 48 4\r# You can also use nrow(weeds) or ncol(weeds) to get them separately\rnames(weeds) # Gives you the column names. \r## [1] \u0026quot;weeds\u0026quot; \u0026quot;soil\u0026quot; \u0026quot;species\u0026quot; \u0026quot;flowers.m3\u0026quot;\r# I use this when I want the exact name for a column when I am writing analyses (you will see later how useful this can be)\rsummary(weeds) # Gives you summary statistics for each column\r## weeds soil species flowers.m3 ## native:24 sandstone:24 Coprosma :16 Min. :13.00 ## weed :24 shale :24 Olearia :16 1st Qu.:22.50 ## Pultenaea:16 Median :33.00 ## Mean :33.81 ## 3rd Qu.:45.50 ## Max. :57.00\r# This will also come in handy later for statistical analysis \rAs you can see, there are many ways to view data within R. Some of these are useful for huge datasets (\u0026gt; 10k rows) as the view() command can put strain on your computer. Using head() or tail() to view aspects of the data is useful as it reduces how much is displayed.\nUse the summary() command with the “insecticide”\u0026quot; dataset to answer the following questions: Question: What is the minimum value for species richness?\n   Expand me...   1\n \rQuestion: What is the maximum value for species richness?\n   Expand me...   20\n \r"
},
{
	"uri": "/data-exploration-and-manipulation/3_columns/",
	"title": "Columns",
	"tags": [],
	"description": "",
	"content": "Another important aspect of R coding syntax is refering to specific columns. This is done by using a $ sign after specifying our dataset and then calling the column. Like so:\nhead(weeds$flowers.m3) # This says to run the head() command but only on the flowers.m3 column\r## [1] 14 17 23 26 35 45\rTry this with some of the other commands above. Note: Some of them will not work and will show NULL. This is because these are designed to view aspects of the data frame (e.g. names() )\nNow we know how to refer to a column, we can fix any issues with importing incorrect data\n# Pretend for a moment our data was input incorrectly\rweeds$species\u0026lt;-factor(weeds$species) # this would simply save the command factor() on the column species to our weeds object.\r# If we wanted an ordered factor, e.g. small \u0026lt; medium \u0026lt; large we can use the following\r#example dataset\rsizes \u0026lt;- factor(c(\u0026quot;small\u0026quot;, \u0026quot;large\u0026quot;, \u0026quot;large\u0026quot;, \u0026quot;small\u0026quot;, \u0026quot;medium\u0026quot;, \u0026quot;medium\u0026quot;)) # creating a single column factor with 3 levels\rsizes\r## [1] small large large small medium medium\r## Levels: large medium small\rsizes \u0026lt;- ordered(sizes, levels = c(\u0026quot;small\u0026quot;, \u0026quot;medium\u0026quot;, \u0026quot;large\u0026quot;)) # ordering levels from small through to large.\r# Note: I did not need to specify column as this is a single column dataset. sizes # Now the factor is ordered.\r## [1] small large large small medium medium\r## Levels: small \u0026lt; medium \u0026lt; large\rweeds$species\u0026lt;-ordered(weeds$species, levels=c(\u0026quot;Pultenaea\u0026quot;, \u0026quot;Olearia\u0026quot;, \u0026quot;Coprosma\u0026quot;)) # This is an example with our data set, but this is nonsensical ordering so dont do this\rBy default, R will always sort in alphabetical order, which can be a pain when graphing. If you want ordered factors, or want to present factors along an X axis in a more logical order then the ordered() command or even factor() command where you specify levels is a good option.\nIf you want to change something to a continuous (numeric, integer etc.) its a little more complicated, but in general R shouldn’t mess this up too often. A quick google search or ?numeric will help answer this.\nOnce we can reference to specific columns we can do interesting things like plot a quick graph.\nplot(weeds$flowers.m3 ~ weeds$soil) # This says to plot a graph with flowers.m3 by (~) soil. \r# You should get a box and whisker plot. hist(weeds$flowers.m3) # hist() is a command that only works on numeric (continuous) columns, and will show you a histogram\rTry replacing the column names in the plot() command to see what types of graphs you get. We will return to graphing at a later date, but this is a quick and easy way to view your data.\nUsing the plot() command with the insecticide dataset answer the following question:\nQuestion: What fragment size has a species richness outlier? (according to the box \u0026amp; whisker plot)\n   Expand me...   Small\n \n"
},
{
	"uri": "/introduction-to-r/05_importing-data/",
	"title": "Importing Data",
	"tags": [],
	"description": "",
	"content": "Now that we have successfully have a project and notebook, we can start to read in data.\nThe first thing with R is that working with normal excel files is quite difficult. So we always work with comma separated values or .CSV files. When saving an excel sheet, just save as and select .csv (comma delimited) as the file type. note: .csv’s can only save a single sheet, not the whole excel workbook\nWhen saving, Excel will inform you that some features may not save with a .csv. This shouldn’t be a problem for you, but read these to make sure.\nImporting our data into R allows us to not only analyse and graph the data, but do manipulations, like create new columns using formulae, rearranging, rename and even removing columns \u0026amp; rows without modifying our original data file. This is incredibly useful to maintain the original raw data, allowing you to share the data and R script with collaborators across a wide range of platforms.\nFor these tutorials, all data will be csv files (unless otherwise specified).\nDownload the “weeds” and “insecticide” datasets from the Datasets tab and save them to your project folder.\n Use the following code within a chunk to enter your data into R:\nread.csv(\u0026quot;weeds.csv\u0026quot;)\rThe file needs to be in your set working directory. If it is not, you need the full filepath. You can skip the working directory step by using the full file path. e.g. “C:/Users/Mitch/Documents/R/weeds.csv”\nThis can be copied from file explorer, but make sure the slash’s are / and not\n This alone will just read the data in its basic form into R. If we want to call on this later we need to save the datafile or “data frame” to a variable of our choosing. By saving different functions in R to a variable/object we reduce the amount of work we need to do later. Instead of typing “group_data_2018_complete.csv” everytime, we can instead just call it “data”, “X” or even “skittles” and type that when we refer to the data.\nI personally would choose something a little more descriptive than just “data”, as it can get confusing when working with multiple data sets\nAssigning a function in R to a variable is one of the most important aspects of coding in R.\n This is done by the following:\nweeds \u0026lt;- read.csv(\u0026#39;weeds.csv\u0026#39;)\r# we simply direct our command, read.csv() to our variable name using an arrow of \u0026lt; and -\r# alternatively, \u0026#39;alt\u0026#39; + \u0026#39;-\u0026#39; is the shortcut for this. \rR will automatically assume that the first row are our column headings. The read.csv() command has this by default. If you want to change this, simply include the header=FALSE argument (like below). Arguments are anything within the brackets of a command that can be added to the command. Even stating your filename in the read.csv is an “argument”\nSimilarly, read.csv() defaults its own row numbers (like excel). You can change this by adding row.names= to the command. If you add =1 it will take the first column as your row numbers/names.\nweeds \u0026lt;- read.csv(\u0026#39;weeds.csv\u0026#39;, header=FALSE) # this will stop the automatic placement of your first row as your column headers. The default for this is TRUE\rweeds \u0026lt;- read.csv(\u0026#39;weeds.csv\u0026#39;, row.names=1) # This places your first column as your row names. Change the number to make a different column your row names\r# This is useful to place site names/numbers as your row numbers. It is basically required when trying to do multivariate (PRIMER) analysis in R.\r# You can combine the two just by adding a comma between them, like so:\rweeds \u0026lt;- read.csv(\u0026#39;weeds.csv\u0026#39;, header=TRUE, row.names=1)\r# we want the first row to be our column names, so we say TRUE for header. # You dont need to do this, as its an assumed default by R...but its good practice\r# For these workshops we will be using R\u0026#39;s default row numbering. So just overwrite your read.csv() without the row.names\rweeds \u0026lt;- read.csv(\u0026#39;weeds.csv\u0026#39;, header=TRUE)\rOnce your data is imported into R and saved as an object, either click the object in the Workspace/environment or use View(weeds)\nWhat is the argument of read.csv() I would need to use to make the sites column my row names?\n ## sites type\r## 1 WAM5 plan\r## 2 WBT1 iso\r## 3 WBT2 rem\r## 4 WBT4 rip\r## 5 WCS2 rip\r## 6 WCS3 iso\r  Answer   row.names=1\nfull command: read.csv(\u0026quot;sitedata.csv\u0026quot;, row.names=1)\n "
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/1_rename/",
	"title": "The Rename Function",
	"tags": [],
	"description": "",
	"content": "By now, if you are like me, you are probably getting annoyed at writing “flowers.m3” or “species.richness” everytime you need to refer to one of those columns. This will happen ALOT with data you enter or obtain from others, as R converts any spaces to fullstops and people tend to capitilise most words. The rename() function allows us to simply rename a column name within our data frame.\nPersonally, this is my favourite function in R as I hate captials, fullstops and other annoying column name problems that slow down coding or generate errors. Trust me, when you spend an hour trying to fix a line of code only to find a single capital letter is missing, you will understand.\ninsert angry/twitching gif here***\nTo do this with dplyr (a tidyverse package) we simply use the following command:\nweeds \u0026lt;- rename(weeds, flowers = flowers.m3)\r# In the brackets we need to specify our data frame (weeds) followed by a second argument specifying the name we want for our column = the name we already have.\r# Again, if you run this by itself it will not save to your data frame, unless you direct it to your data frame variable using the \u0026lt;- \rPretty simple and straightforward.\nIf you want to rename multiple columns, this is a pretty simple addition. For demonstrating purposes, I am going to rename all the columns of weeds to nonsensical crap.\ngrocerylist \u0026lt;- rename(weeds, !!c(coopers = \u0026quot;flowers\u0026quot;, asahi = \u0026quot;species\u0026quot;, vb = \u0026quot;soil\u0026quot;, littlecreatures = \u0026quot;weeds\u0026quot;))\r# to specify multiple variables/columns, we simply concatenate (put inside brackets with c before them) the multiple arguments into a string. \rAlternatively, you can specify the variables to change separately in a line before the rename() command.\nbeer \u0026lt;- c(coopers = \u0026quot;flowers\u0026quot;, asahi = \u0026quot;species\u0026quot;, vb = \u0026quot;soil\u0026quot;, littlecreatures = \u0026quot;weeds\u0026quot;)\rgrocerylist \u0026lt;- rename(weeds, !!beer)\r"
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/",
	"title": "Manipulating data",
	"tags": [],
	"description": "",
	"content": " As previously mentioned, one of the extremely useful and time saving parts of R is manipulating your data without touching your original spreadsheet. Manipulating your data within the R environment allows us to generate entirely new datasets based off our raw data, without modifying the original document. This means no more multi-sheet excel workbooks, no more opening excel to generate a new column, this can all be done in R. This is really beneficial when collaborating with other researchers, group members or supervisors since all we are required to do is send the raw data sheet and the R document. They can follow the code to see what is happening and run everything directly from the original data.\nManipulation of data can be done with the base R language (everything we have done so far) or with packages in the Tidyverse library, such as \u0026ldquo;dplyr\u0026rdquo; and \u0026ldquo;tidyr\u0026rdquo;. `Tidyverse simplifies the language of coding and offers powerful tools for data manipulation and graphing.\nMake sure you have loaded tidyverse with the library() command before attempting any functions.\n For the rest of the course from here on out, there will be many arguments of functions that will be left out. If you want to learn about other customisation options for your code, or are lost at any point, use the \u0026ldquo;Help\u0026rdquo; tab in R studio or type \u0026rdquo;?\u0026rdquo; followed by the name of the function. e.g. ?rename.\nOtherwise, the internet is a awesome resource for R help.\nContent:  The Rename Function  By now, if you are like me, you are probably getting annoyed at writing “flowers.m3” or “species.richness” everytime you need to refer to one of those columns. This will happen ALOT with data you enter or obtain from others, as R converts any spaces to fullstops and people tend to capitilise most words. The rename() function allows us to simply rename a column name within our data frame. Personally, this is my favourite function in R as I hate captials, fullstops and other annoying column name problems that slow down coding or generate errors.\n The Mutate Function  One of the most common data manipulations is adding a new column to your dataset. This is great for transforming data, while also keeping the original. This could be used to combine multiple columns into one or perform mathematical calculations involving multiple columns with the results in a separate column. We will start out with a few simple methods in base R, and then move to the dplyr method. ##Log Transformation##\rweeds$log_flowers \u0026lt;- log(weeds$flowers) # Base R\rweeds \u0026lt;- mutate(weeds, log_flowers = log(flowers)) # Dplyr\r# Each of these creates a new column which is the log of the flowers column.\n The Filter Function  The filter() command is used to remove rows from your data. This can be useful for removing zeros or “no data/NA’s”, or for restricting certain variables in a dataset for an analysis. This follows the similar syntax as mutate() whereby we specify what dataset we want to filter, followed by how we want to filter. #The following examples will just keep overwriting the new object \u0026quot;weeds_filtered\u0026quot;\rweeds_filtered \u0026lt;- filter(weeds, weeds == \u0026quot;native\u0026quot;) # Gives us only the rows which are exactly \u0026quot;native\u0026quot; in the weeds column.\n The Select Function  The select() function is used to select specific columns within your data and save them as a new data frame. You can use this if you have a large dataset and only want to use a few of the columns, to keep it simple and tidy. Or, you may want to take a column or two from multiple different datasets and combine them. weeds_select \u0026lt;- select(weeds, soil) This simply creates the weeds_select dataset, seleting one column - “soil”.\n Joining Data  One of the most frequent data manipulations for working within R is joining multiple data sets together. The most common example of this is combining species abundance (or some other variable of interest) with external sources on the environmental conditions, such as BOM data (temperature, precipitation etc.) or GPS data. To do most statistical analyses, data needs to be in the same data frame. So joining the datasets is an “easy” way to do so outside of excel.\n Removing Items  By now, we have quite a few objects in our R environment that aren’t being used. The remove() command does exactly that….removes objects from the R environment. This helps for making things nice and tidy, specifically in our environment window. remove(frogfull, froginner, enviro_filter) # Removes all three objects we just generated\rUse this in the console to “one off” remove an item. … that’s all, NEXT!\n The Summarise Function  This is an extremely useful function that lets you create different summaries of columns. You can also nest other functions within it to apply them to your columns. sum_data \u0026lt;- summarise(weeds, mean(flowers)) # We\u0026#39;ll start simple. Generates the mean of the flower column\rsum_data \u0026lt;- summarise(group_by(weeds, species), mean(flowers)) # Using the group_by() function within summarise lets you get summaries for groups, in this case \u0026quot;species\u0026quot;\rsum_data \u0026lt;- summarise(group_by(weeds,species, soil), mean(flowers), sd(flowers), se=sd(flowers/sqrt(n())))\r# Grouped by with species \u0026amp; soil, generating mean, standard deviation \u0026amp; standard error of flowers\rThe last example generates the mean, sd and se for each factor combination in our dataset.\n The Pipe Function  This lets you run multiple different functions on one dataset without having to use the intermediate steps you would have to use in base R. You start with the data you want to apply the functions to, followed by a pipe %\u0026gt;%. After each pipe you must go to the next line. This is useful for large messy functions with multiple nested parts. It separates everything out and makes it easier to follow.\n "
},
{
	"uri": "/statistical-analysis/",
	"title": "Statistical Analysis",
	"tags": [],
	"description": "",
	"content": " Statistical Analysis Analysing your data in simple and meaningful ways.\n"
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/2_mutate/",
	"title": "The Mutate Function",
	"tags": [],
	"description": "",
	"content": "One of the most common data manipulations is adding a new column to your dataset. This is great for transforming data, while also keeping the original. This could be used to combine multiple columns into one or perform mathematical calculations involving multiple columns with the results in a separate column.\nWe will start out with a few simple methods in base R, and then move to the dplyr method.\n##Log Transformation##\rweeds$log_flowers \u0026lt;- log(weeds$flowers) # Base R\rweeds \u0026lt;- mutate(weeds, log_flowers = log(flowers)) # Dplyr\r# Each of these creates a new column which is the log of the flowers column.\r## Basic math functions##\rweeds_mutate \u0026lt;- mutate(weeds, flowers2 = flowers*2) # Simple multiplication of the flowers column by 2\rweeds_mutate \u0026lt;- mutate(weeds, flowers_combined = flowers + flowers2) # This is a useless example but its just to show you how to combine multiple columns. weeds_mutate \u0026lt;- mutate(data, binary = soil == \u0026quot;sandstone\u0026quot;) # Using boolean logic to create a column called \u0026quot;binary\u0026quot; where soil is exactly (hence double =\u0026#39;s) sandstone. weeds_mutate \u0026lt;- mutate(data, flowers2 = flowers*2,\rbinary = soil == \u0026quot;sandstone\u0026quot;) # You can also perform the functions multiple times on the same data within one line. \rThe arguments of mutate() are simply the name of the data frame followed by any number of expressions that create new variables.\nYou will notice throughout the mutate() commands that we have performed functions, creating new columns, while preserving the original. If you wish to drop the original column, simply use the transmute() command.\nUse the Mutate() command on the insecticide dataset to answer the following question\nQuestion: What is the square of the number in the last row of species.richness (row 42)?\n{1:SHORTANSWER:=255}\n"
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/3_filter/",
	"title": "The Filter Function",
	"tags": [],
	"description": "",
	"content": "The filter() command is used to remove rows from your data. This can be useful for removing zeros or “no data/NA’s”, or for restricting certain variables in a dataset for an analysis.\nThis follows the similar syntax as mutate() whereby we specify what dataset we want to filter, followed by how we want to filter.\n#The following examples will just keep overwriting the new object \u0026quot;weeds_filtered\u0026quot;\rweeds_filtered \u0026lt;- filter(weeds, weeds == \u0026quot;native\u0026quot;) # Gives us only the rows which are exactly \u0026quot;native\u0026quot; in the weeds column. weeds_filtered \u0026lt;- filter(weeds, weeds != \u0026quot;weed\u0026quot;) # This gives us the same result as their are only two levels of that column. The != means \u0026quot;not equal to\u0026quot;\rweeds_filtered \u0026lt;- filter(weeds ,flowers \u0026gt; 20) # Flowers greater than 20 m3\rSo far, we have covered renaming columns, adding new columns and filtering by rows. The next two commands are focused on selecting specific columns and creating new data tables.\n"
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/4_select/",
	"title": "The Select Function",
	"tags": [],
	"description": "",
	"content": "The select() function is used to select specific columns within your data and save them as a new data frame. You can use this if you have a large dataset and only want to use a few of the columns, to keep it simple and tidy. Or, you may want to take a column or two from multiple different datasets and combine them.\nweeds_select \u0026lt;- select(weeds, soil) \rThis simply creates the weeds_select dataset, seleting one column - “soil”. As with most tidyverse functions we need to specify the dataset immediately after writing the select function. From here, its simple changes to do use select in new ways\nweeds_select \u0026lt;- select(weeds,c(soil, species)) # select two columns, \u0026quot;soil\u0026quot; and \u0026quot;species\u0026quot;\rweeds_select \u0026lt;- select(weeds,c(2:4)) # select columns using numbers. In this case, select columns 2 through to 4.\rweeds_select \u0026lt;- select(weeds, c(soil:flowers)) # select columns \u0026quot;soil\u0026quot; through to \u0026quot;flowers\u0026quot;\rweeds_select \u0026lt;- select(weeds, -soil) # remove \u0026quot;soil\u0026quot;\r# similar syntax applys for removing multiple columns, just place a - infront e.g. select(weeds, -c(2:4))\rweeds_select \u0026lt;- select(weeds, starts_with(\u0026quot;s\u0026quot;)) # select any column whose name starts with S. \rThere are many more like this above example, like “ends_with”, “contains” and “matches” all which refer to the column names.\nuse the help window ?select for more useful functions with select()\n "
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/5_joins/",
	"title": "Joining Data",
	"tags": [],
	"description": "",
	"content": "One of the most frequent data manipulations for working within R is joining multiple data sets together. The most common example of this is combining species abundance (or some other variable of interest) with external sources on the environmental conditions, such as BOM data (temperature, precipitation etc.) or GPS data.\nTo do most statistical analyses, data needs to be in the same data frame. So joining the datasets is an “easy” way to do so outside of excel.\nFor this exercise, we will be working with the BIOL365 Frog Data to combine the species matrix with environmental data.\nDownload the “frogs.csv” and “frog_environmental.csv”\u0026quot; files and read them in to R without the row.names argument\n frogsp\u0026lt;-read.csv(\u0026quot;frogs.csv\u0026quot;, header=TRUE)\renviro\u0026lt;-read.csv(\u0026quot;frog_environmental.csv\u0026quot;, header=TRUE)\rFor a complete join of both datasets, when there are the same number of rows in the exact same order, we can use the bind_cols() function.\nfrogcombine \u0026lt;- bind_cols(frogsp, enviro) # In this example the \u0026quot;site\u0026quot; column has been added twice\rThere is a bind_rows() that will add rows to the bottom of a dataset, using the same syntax.\nWhile bind_cols() and bind_rows() are “cool”, they are limited in their usefulness. I find the most useful function is left_join().\nfrogjoin \u0026lt;- left_join(frogsp, enviro, by=\u0026quot;Site\u0026quot;) # This will join two datasets by a similar column (Site). \rThis will join the second dataset (enviro) to the first data set based on the shared column. right_join() will do the opposite, joining frogsp to enviro. Its pretty useless, just use left_join() remember to always put the data frame you want to keep first.\n# We can use the dim() to view the dimensions of the data\rdim(frogsp) # 11 columns\r## [1] 42 11\rdim(enviro) # 16 columns\r## [1] 42 16\rdim(frogjoin) # 26 colums (11 + 16 minus the 1 in common)\r## [1] 42 26\rBoth of these examples so far have required the same rows for each dataset. Sometimes we might have more information in one dataset then we do in the other. For this dataset we don’t have this issue, so lets quickly create the issue to demonstrate.\nWe will simply use the filter() command to filter for rows that contain a value in the “Temp” column. We have 4 rows that have an NA in “Temp” so we will use a != (not equal to) to select all rows that are not equal to NA\nenviro_filter \u0026lt;- filter(enviro, Temp != \u0026quot;NA\u0026quot;) # This removes sites 14, 15, 35 \u0026amp; 36\rdim(enviro_filter)\r## [1] 38 16\r# Now we can try the two new join types\rfroginner \u0026lt;- inner_join(frogsp, enviro_filter, by=\u0026quot;Site\u0026quot;) # Join data. Retain only rows that occur in both data sets\rdim(froginner) # 38 rows\r## [1] 38 26\rfrogfull \u0026lt;- full_join(frogsp, enviro_filter, by=\u0026quot;Site\u0026quot;) # Join data. Retain all values, all rows\rdim(frogfull)\r## [1] 42 26\rYou can also use semi_join() to combine all rows that have a match in the second dataset, or anti_join() to combine all rows that do not match have a match in the second dataset (this ones a little weird).\nI still find myself using left_join 90% of the time though.\n"
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/6_remove/",
	"title": "Removing Items",
	"tags": [],
	"description": "",
	"content": "By now, we have quite a few objects in our R environment that aren’t being used. The remove() command does exactly that….removes objects from the R environment. This helps for making things nice and tidy, specifically in our environment window.\nremove(frogfull, froginner, enviro_filter) # Removes all three objects we just generated\rUse this in the console to “one off” remove an item.\n… that’s all, NEXT!\n"
},
{
	"uri": "/statistical-analysis/analysis-of-variance/",
	"title": "Analysis of Variance",
	"tags": [],
	"description": "",
	"content": " Analysis of Variance To begin our foray into statistics in R, we will start with the most basic and useful analysis, Analysis of Variance (ANOVA). An ANOVA is used to test the effect of 1 or more categorical explanatory variables (X) on a continuous response variable (Y). The ANOVA tests the difference between the factors variance (distance from the grand mean) compared to the error variance. The variance for each point is squared and added together to generate the sum of squares, which is then used to generate the F ratio. The F ratio is then compared to a critical value from a table of values (based of degrees of freedom) to determine the level of significance, or P value.\nIn these tutorials, we will be using a variety of datasets to test each analysis. These will be specified within a \u0026ldquo;instruction\u0026rdquo; block before the requisite code chunk.\nContent:  Analysis of Variance  The first step for conducting an ANOVA in R is to create an ANOVA object. There are two ways of doing this, using the lm() command, and using the aov() command. For simplicity we will be using the aov() command now, but we will get to the lm() object later. By using the aov() command, we can create an object that tells summary(), plot() or any other commands that the object is specifically for an ANOVA and as such, will be treated as one.\n Assumptions  All statistical tests need to make various assumptions about your data when conducting the test. This is Content: Normality Normality can be tested in two basic ways. Through visual inspection of residuals in a normal quantile (QQ) plot and histogram, OR, through a mathematical test such as a shapiro-wilks test. To produce the two graphs for visual inspection of residuals we use the following commands: plot(weeds.aov, 2) # Normal quantile plot\rThe normal qq plot should display the residuals along the dotted line in a straight manner.\n Viewing results  Once we know our data is normal and we have our aov() object, we can use one of two commands on this object to generate our statistical result. The normal way to do so is to use the anova() command. anova(weeds.aov) # run an anova on the object\r## Analysis of Variance Table\r## ## Response: flowers\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2368.6 1184.\n Two-factor ANOVAs  To conduct an two-factor ANOVA is pretty straightforward. weeds.aov2 \u0026lt;- aov(flowers ~ species + soil, data = weeds) # two-factor anova (without interaction)\rsummary(weeds.aov2)\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2369 1184.3 9.272 0.000436 ***\r## soil 1 239 238.5 1.867 0.178720 ## Residuals 44 5620 127.7 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThis example constructs an ANOVA with two factors, but does not include the interaction term.\n Tukeys HSD  All of our analyses so far have showed us that species has an influence on flower abundance. But without conducting an extra test, we cannot be certain which species are statistically significant from each other when it comes to their effect on flower abundance TukeyHSD(weeds.aov) ## Tukey multiple comparisons of means\r## 95% family-wise confidence level\r## ## Fit: aov(formula = flowers ~ species, data = weeds)\r## ## $species\r## diff lwr upr p adj\r## Olearia-Coprosma 12.\n "
},
{
	"uri": "/statistical-analysis/analysis-of-variance/1_aov/",
	"title": "Analysis of Variance",
	"tags": [],
	"description": "",
	"content": "The first step for conducting an ANOVA in R is to create an ANOVA object. There are two ways of doing this, using the lm() command, and using the aov() command. For simplicity we will be using the aov() command now, but we will get to the lm() object later.\nBy using the aov() command, we can create an object that tells summary(), plot() or any other commands that the object is specifically for an ANOVA and as such, will be treated as one.\nThe syntax for almost all analyses in R is the same. Within our analysis command (aov() in this example) we write a line equation for our analysis:\ny ~ x\nThis is simply your response variable (Y) and explanatory variable(s) (X) separated by a tilde ~. The tilde acts as an = sign for the analysis.\nFor this analysis, we will be using the weeds dataset.\n weeds.aov \u0026lt;- aov(flowers ~ species, data=weeds) # flowers (Y variable) ~ species (X variable), then data = weeds to direct it to our dataframe. \rAs per usual, name your newly created object something that will remind you of what it is. I tend to name it something to remind me of the dataset as well as the statistical technique. Generally dataset.analysis.\n Now we have our aov() object, it is good practice to test the assumptions before we look at the results of the analysis. Having this workflow in place will hopefully prevent you from conveniently “forgetting” assumptions after seeing a significant result.\n"
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/7_summarise/",
	"title": "The Summarise Function",
	"tags": [],
	"description": "",
	"content": "This is an extremely useful function that lets you create different summaries of columns. You can also nest other functions within it to apply them to your columns.\nsum_data \u0026lt;- summarise(weeds, mean(flowers)) # We\u0026#39;ll start simple. Generates the mean of the flower column\rsum_data \u0026lt;- summarise(group_by(weeds, species), mean(flowers)) # Using the group_by() function within summarise lets you get summaries for groups, in this case \u0026quot;species\u0026quot;\rsum_data \u0026lt;- summarise(group_by(weeds,species, soil), mean(flowers), sd(flowers), se=sd(flowers/sqrt(n())))\r# Grouped by with species \u0026amp; soil, generating mean, standard deviation \u0026amp; standard error of flowers\rThe last example generates the mean, sd and se for each factor combination in our dataset. This is pretty useful, particularly for generating bar graphs.\nHowever, its a little complex and can be in a much nicer format.\nUse the summarise() function on the “insecticide”\u0026quot; dataset to answer the following question\nQuestion: In the large fragment, what is the median species richness\n   Answer   12\n \r"
},
{
	"uri": "/data-exploration-and-manipulation/manipulating-data/8_pipe/",
	"title": "The Pipe Function",
	"tags": [],
	"description": "",
	"content": "This lets you run multiple different functions on one dataset without having to use the intermediate steps you would have to use in base R.\nYou start with the data you want to apply the functions to, followed by a pipe %\u0026gt;%. After each pipe you must go to the next line.\nThis is useful for large messy functions with multiple nested parts. It separates everything out and makes it easier to follow.\nA pipe is simply a \u0026gt; nested within two percentage, %, symbols. The keyboard shortcut for this is Ctrl + SHIFT + M\nsum_data \u0026lt;- weeds %\u0026gt;% group_by(species, soil) %\u0026gt;% summarise(max(flowers))\rYou simply start with the data you want to apply the functions to, followed by a pipe. After each pipe you must go to the next line (sorta).\nIn this example, we grouped the data by species and soil, then performed the summarise function to generate the max number for each combination\nYou will notice, that because we specified the data in the first line, we did not have to specify the data in the other lines, only the columns\n new_data \u0026lt;- weeds %\u0026gt;% mutate(binary = soil == \u0026quot;sandstone\u0026quot;) %\u0026gt;% filter(weeds == \u0026quot;native\u0026quot;)\rAs you can see, we can do this with most of the functions we have already learnt. This above example will generate a binary outcome (true/false) for soil with TRUE as “sandstone”. Followed by filtering for “native” weeds. This will generate a a dataset with native weeds that have a true/false outcome based on soil.\nPiping is incredibly useful and much easier to read. It is a function I keep forgetting to use, until I look at my code later on, full of regrets. It shortens and simplifies code alot.\n"
},
{
	"uri": "/statistical-analysis/analysis-of-variance/assumptions/",
	"title": "Assumptions",
	"tags": [],
	"description": "",
	"content": " All statistical tests need to make various assumptions about your data when conducting the test. This is\nContent:  Normality  Normality can be tested in two basic ways. Through visual inspection of residuals in a normal quantile (QQ) plot and histogram, OR, through a mathematical test such as a shapiro-wilks test. To produce the two graphs for visual inspection of residuals we use the following commands: plot(weeds.aov, 2) # Normal quantile plot\rThe normal qq plot should display the residuals along the dotted line in a straight manner. In this example, it is pretty straight :)\n Homogeneity of Variance  Homogeneity of variance is the other main assumption we are concerned with when conducting an ANOVA. Homogeneity of variance is the assumption that the variance between groups is relatively even. That is to say, all groups have similar variation between them. Similar to the assumption of normality, there are two ways to test homogeneity, a visual inspection of residuals and a statistical test. To conduct a visual inspection of the residuals we simply use the following:\n "
},
{
	"uri": "/statistical-analysis/analysis-of-variance/assumptions/1_normality/",
	"title": "Normality",
	"tags": [],
	"description": "",
	"content": "Normality can be tested in two basic ways. Through visual inspection of residuals in a normal quantile (QQ) plot and histogram, OR, through a mathematical test such as a shapiro-wilks test.\nTo produce the two graphs for visual inspection of residuals we use the following commands:\nplot(weeds.aov, 2) # Normal quantile plot\rThe normal qq plot should display the residuals along the dotted line in a straight manner. In this example, it is pretty straight :)\nPlotting the aov() object will generate 4 plots. The QQ plot is the second, so we can just specify the second one to avoid the other 3.\nTry removing the 2 and comma and see what the plot command does on its own. It does this because it is being applied to a statistical object (i.e. aov() ).\nThe plot command (without specifying 1-4) may require you to press “ENTER” in the console for each plot. Make sure to look in the console after running this command\n We can also produce a histogram of the residuals:\nhist(weeds.aov$residuals) # Histogram of residuals\rTo produce a histogram of the residuals, we simply need to specify the residuals column of our aov() object. Simple!\nThe reason this works is because the aov() object contains its own column for residuals (amongst other things). Try running str() on weeds.aov to see what columns it contains. The second one should be the residuals.\nSomething “fun” to do, is to combine both of the graphs in the same window. This produces the same style of output we would get in JMP.\n## This may not work in a default notebook ##\rpar(mfrow = c(1,2)) # This code put two plots in the same window\rhist(weeds.aov$residuals)\rplot(weeds.aov, 2)\rIf your using a notebook, the par() command wont work. Click the settings cog and select “chunk output in console”.\nIf your future graphs keep using this two plot window, use the following command in the console to stop it. dev.off()\n Running a shapiro-wilks test is a similar story. To produce a shapiro-wilks test requires the following code:\nshapiro.test(weeds.aov$residuals) # run a shapiro-wilks test on the residuals column of our anova object using the shapiro.test() function\r## ## Shapiro-Wilk normality test\r## ## data: weeds.aov$residuals\r## W = 0.98282, p-value = 0.6993\rWith a shapiro-wilks test, if the result is significant, this means our data is NOT-NORMAL. In our case our data is normal.\nYou should be aware that the shapiro-wilks test is very sensitive to departures from normality and is often considered a “harsh” test. There are many other options for testing normality through an empirical test. Most people prefer to stick with visual inspections of residuals to avoid this.\nWhat is the shapiro-wilks p-value for an anova of species richness and fragment in the “Insecticide” dataset\n   Answer   0.01113\n \r"
},
{
	"uri": "/statistical-analysis/analysis-of-variance/assumptions/2_homogeneity/",
	"title": "Homogeneity of Variance",
	"tags": [],
	"description": "",
	"content": "Homogeneity of variance is the other main assumption we are concerned with when conducting an ANOVA. Homogeneity of variance is the assumption that the variance between groups is relatively even. That is to say, all groups have similar variation between them. Similar to the assumption of normality, there are two ways to test homogeneity, a visual inspection of residuals and a statistical test.\nTo conduct a visual inspection of the residuals we simply use the following:\nplot(weeds.aov, 1) # using plot number 1 this time\rHeterogenous variances are indicated by a non-random pattern in the residuals vs fitted plot. We look for an even spread of residuals along the Y axis for each of the levels in the X axis. We know species contains 3 levels (“Comprosma”, “Oleria” \u0026amp; “Pultenaea”) so we should see three columns of dots, with an even spread along the Y axis.\nThe other way to test this is to use a statistical test, such as a Cochrans or Bartletts test. For this module, we will be taking a departure from the typical Cochran’s Test as there are other tests that (in my personal opinion) are more useful and WAY easier to conduct in R. The first of these will be a Bartlett’s test.\nbartlett.test(flowers ~ species, data = weeds)\r## ## Bartlett test of homogeneity of variances\r## ## data: flowers by species\r## Bartlett\u0026#39;s K-squared = 0.15957, df = 2, p-value = 0.9233\rSimple and easy!\nThis shows us that the variances are homogenous (i.e. a non-significant P value). The reason we may not use a Bartlett’s test all of the time is because it is highly sensitive to departures from normality (i.e. non-normal datasets). If we suspect our data is not-normal or is slightly not-normal and want to test homogeneity of variance anyways, we can use a Levene’s Test to account for this. I suggest reading up on the differences between bartlett’s and levene’s tests before using levene’s. Here is how to do it anyway:\nlibrary(car) # install the car package for this test\rleveneTest(flowers ~ species, data=weeds)\r## Levene\u0026#39;s Test for Homogeneity of Variance (center = median)\r## Df F value Pr(\u0026gt;F)\r## group 2 0.3131 0.7327\r## 45\rAgain, simple and easy to use. Our P value is not significant which agrees with the Bartlett’s test result.\nWhat is the bartlett p-value for the species richness by fragment analysis in the Insecticide dataset?\n   Answer   0.1022\n \n"
},
{
	"uri": "/statistical-analysis/analysis-of-variance/3_anovaresults/",
	"title": "Viewing results",
	"tags": [],
	"description": "",
	"content": "Once we know our data is normal and we have our aov() object, we can use one of two commands on this object to generate our statistical result. The normal way to do so is to use the anova() command.\nanova(weeds.aov) # run an anova on the object\r## Analysis of Variance Table\r## ## Response: flowers\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2368.6 1184.31 9.0966 0.0004811 ***\r## Residuals 45 5858.7 130.19 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# WOO! Significance!\rAlternatively we can use the summary() command on our aov() object to generate the same result. For most other analyses, such as linear regressions and mixed models, we will use the summary() command exclusively.\nBecause we created an aov() object, the summary() command automatically does an ANOVA.\nsummary(weeds.aov) # print a summary of the object. In this case the summary is an anova\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2369 1184.3 9.097 0.000481 ***\r## Residuals 45 5859 130.2 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rUsing the summary() command, what is the F-value for fragment in the Insecticide ANOVA?\n   Answer   14.94\n \n"
},
{
	"uri": "/statistical-analysis/analysis-of-variance/4_twofactors/",
	"title": "Two-factor ANOVAs",
	"tags": [],
	"description": "",
	"content": "To conduct an two-factor ANOVA is pretty straightforward.\nweeds.aov2 \u0026lt;- aov(flowers ~ species + soil, data = weeds) # two-factor anova (without interaction)\rsummary(weeds.aov2)\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2369 1184.3 9.272 0.000436 ***\r## soil 1 239 238.5 1.867 0.178720 ## Residuals 44 5620 127.7 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThis example constructs an ANOVA with two factors, but does not include the interaction term. If we want the interaction term, simply replace the + sign with an asterisk * .\nweeds.aov2 \u0026lt;- aov(flowers ~ species * soil, data = weeds) # two-factor anova (with interaction)\rsummary(weeds.aov2)\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2369 1184.3 9.102 0.00052 ***\r## soil 1 239 238.5 1.833 0.18301 ## species:soil 2 155 77.5 0.596 0.55574 ## Residuals 42 5465 130.1 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rIncluding the asterisk tells the formula to multiply both of the factors creating the interaction factor. It will automatically produce the results for factors independantly as well as the interaction term.\nDon’t forget to check your assumptions\nEverything stays the same for assumptions except the following modifications to Bartlett’s and Levene’s Tests.\nbartlett.test(flowers ~ interaction(species, soil), data = weeds) # Add the interaction() argument to correctly analyse an interaction term\r## ## Bartlett test of homogeneity of variances\r## ## data: flowers by interaction(species, soil)\r## Bartlett\u0026#39;s K-squared = 5.3304, df = 5, p-value = 0.3769\rleveneTest(flowers ~ species * soil, data = weeds) # same syntax as the normal formula\r## Levene\u0026#39;s Test for Homogeneity of Variance (center = median)\r## Df F value Pr(\u0026gt;F)\r## group 5 0.81 0.5492\r## 42\rTransformations\rThere are two methods to transform your response (Y) variable for an analysis.\nUse a data manipulation technique such as mutate() to create a new column; or\rTransform the variable within the analysis formula (see below)\r\rFor this example, we will be log transforming the flowers column within the weeds dataset.\nNOTE: THIS MAKES NO SENSE AS IT IS NORMAL data. IT IS JUST AN EXAMPLE!\n## Mutate Option ##\rweeds \u0026lt;- mutate(weeds, logflowers = log(flowers)) # create new column called \u0026quot;logflowers\u0026quot;\r## Formula option ##\rweeds.aov.log \u0026lt;- aov(log(flowers) ~ species * soil, data = weeds) # log(flowers) as our Y variable tells the anova to use a log transformed response.\rsummary(weeds.aov.log)\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2.842 1.4211 11.158 0.00013 ***\r## soil 1 0.239 0.2387 1.874 0.17831 ## species:soil 2 0.247 0.1234 0.969 0.38792 ## Residuals 42 5.349 0.1274 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rIf you are testing assumptions, you must run the aov() (or general analysis) again with the new transformation and then extract residuals.\n shapiro.test(log(weeds.aov$residuals)) #### DO NOT DO THIS!! ####\r## ## Shapiro-Wilk normality test\r## ## data: log(weeds.aov$residuals)\r## W = 0.95759, p-value = 0.4422\rshapiro.test(weeds.aov.log$residuals) # Do this! #\r## ## Shapiro-Wilk normality test\r## ## data: weeds.aov.log$residuals\r## W = 0.97792, p-value = 0.4951\rSee how those are different? The same thing applies to square root (sqrt) or square/cubic transformations (^2, ^3).\nConstruct a Two-factor ANOVA (with interaction) on the Insecticide dataset and answer the following:\n1. Is the data normal?\n   Answer   NO\n \n2. What is the p-value for the Bartlett’s test?\n   Answer   0.1339\n \n3. Without transforming to normalise, what is the p-value for the interaction term?\n   Answer   0.03275\n \n\r"
},
{
	"uri": "/statistical-analysis/analysis-of-variance/5_tukeys/",
	"title": "Tukeys HSD",
	"tags": [],
	"description": "",
	"content": "All of our analyses so far have showed us that species has an influence on flower abundance. But without conducting an extra test, we cannot be certain which species are statistically significant from each other when it comes to their effect on flower abundance\nTukeyHSD(weeds.aov) \r## Tukey multiple comparisons of means\r## 95% family-wise confidence level\r## ## Fit: aov(formula = flowers ~ species, data = weeds)\r## ## $species\r## diff lwr upr p adj\r## Olearia-Coprosma 12.6250 2.84785 22.40215 0.0084638\r## Pultenaea-Coprosma 16.4375 6.66035 26.21465 0.0005330\r## Pultenaea-Olearia 3.8125 -5.96465 13.58965 0.6149669\rThis is showing us whether the two compared means are significantly different from each other (p adj).\nThis will give us the print out for the whole analysis. If we want only one factor to be displayed, simply include the which =  agument and specify what factor\nTukeyHSD(weeds.aov, which = \u0026quot;species\u0026quot;) # this will give us only the species column\r## Tukey multiple comparisons of means\r## 95% family-wise confidence level\r## ## Fit: aov(formula = flowers ~ species, data = weeds)\r## ## $species\r## diff lwr upr p adj\r## Olearia-Coprosma 12.6250 2.84785 22.40215 0.0084638\r## Pultenaea-Coprosma 16.4375 6.66035 26.21465 0.0005330\r## Pultenaea-Olearia 3.8125 -5.96465 13.58965 0.6149669\rWhile handy and quick, its hard to interpret the print out of this test, particularly in analyses with multi-leveled factors.\nThe following Tukeys HSD test comes from the package agricolae. I personally only use this package for the Tukeys HSD letter report function.\nlibrary(agricolae)\rHSD.test(weeds.aov, \u0026quot;species\u0026quot;, console=TRUE) # HSD.test() requires you to state the factor, as well as print the output to the console (console=TRUE)\r## ## Study: weeds.aov ~ \u0026quot;species\u0026quot;\r## ## HSD Test for flowers ## ## Mean Square Error: 130.1931 ## ## species, means\r## ## flowers std r Min Max\r## Coprosma 24.1250 11.13478 16 13 52\r## Olearia 36.7500 12.08580 16 16 55\r## Pultenaea 40.5625 10.97858 16 20 57\r## ## Alpha: 0.05 ; DF Error: 45 ## Critical Value of Studentized Range: 3.427507 ## ## Minimun Significant Difference: 9.77715 ## ## Treatments with the same letter are not significantly different.\r## ## flowers groups\r## Pultenaea 40.5625 a\r## Olearia 36.7500 a\r## Coprosma 24.1250 b\rAs mentioned, this specific Tukey’s function can only do a single specified factor (to my knowledge). These Tukey’s tests are options for single factor significance. For an interaction significance, you will need to consider alternative post-hoc methods.\nAn interesting method of visualising the interaction term is using the interaction.plot() command. Specifying the three columns you want to see. I find this useful for linear regressions, not so much for categorical data.\ninteraction.plot(weeds$species, weeds$weeds, weeds$flowers)\rIn this example, we see the increase in number of flowers, with a markedly higher increase in native. The significant increase in species is only present in Oleria in native areas.\n"
},
{
	"uri": "/statistical-analysis/6_linearregession/",
	"title": "Linear Regression",
	"tags": [],
	"description": "",
	"content": "Linear regression is one of the most highly used statistical techniques in all of life and earth sciences. It is used to model the relationship between a response (Y) variable and a explanatory (X) variable. A linear regression is a special case of a linear model whereby both the response and explanatory variables are continuous. The ANOVA we just conducted is still considered as a linear model since the response variable is a linear (additive) combination of the effects of the explanatory variables.\nSince we have already conducted an ANOVA, a linear model will be a peice of cake!\nFor this, we will be using the tadpoles.csv dataset.\n str(tadpoles) # three columns, all continuous\r## \u0026#39;data.frame\u0026#39;: 24 obs. of 3 variables:\r## $ reeds : int 1 1 1 1 1 1 1 1 2 2 ...\r## $ pondsize : int 45 60 20 45 56 16 37 49 50 16 ...\r## $ abundance: int 120 201 136 128 178 55 156 150 89 25 ...\rAutomatically, upon reading the tadpoles dataset, we have an issue. Our reeds column should actually be a category, so we need to read that in as a factor. There is argument here for reeds to be ordinal, but for ease of interpretation, we will stick to just a factor.\nMake reeds into a factor\n Once everything is input correctly, we can begin our analysis\ntadpoles.lm \u0026lt;- lm(abundance ~ pondsize, data = tadpoles) # constructing a linear model\rThe lm() command creates a linear model object. In this example we are testing the effect of pondsize on tadpole abundance using a linear regression.\nIt is worth noting that the lm() command can be used to perform an anova, but the aov() command cannot be used for regressions. Give it a try by using lm on our last analysis and use the anova command, as well as the summary() command on the created object.\nsummary(tadpoles.lm) # summarising the newly created linear model object\r## ## Call:\r## lm(formula = abundance ~ pondsize, data = tadpoles)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -73.546 -29.752 -8.026 37.978 77.652 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 23.8251 25.8455 0.922 0.36662 ## pondsize 1.7261 0.5182 3.331 0.00303 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 49.42 on 22 degrees of freedom\r## Multiple R-squared: 0.3352, Adjusted R-squared: 0.305 ## F-statistic: 11.09 on 1 and 22 DF, p-value: 0.003032\rThe estimates for the coefficients give you the slope and the intercept (much like JMP). In this example, the regression equation would be:\n Abundance = 23.8251 + 1.7261*pondize + error \rThe summary() printout gives us a lot of useful information, so we need to narrow down what is most important. The t-value and p-value for each coefficient indicate significance. We dont really care about the intercept. What we do care about is if the other coefficient (pondsize) is significant, indicating an effect of the explanatory variable on the reponse. Because of the positive estimate (1.7261) we can identify that an increase in pondsizeis associated with a significant increase in tadpole abundance.\nWhile the t and p-values indicate a significant association, the R^2 value tells us the strength of the association. In this case, the proportion of variation explained by the explanatory variable is 33.52%.\nAssumptions\rTo test assumptions for linear regression, we need to test the same assumptions we tested for the ANOVA. The only slight exception here is the pattern/appearance of the residuals in the fitted v.s residuals plot AND, we cant use bartlett’s or levene’s tests.\nplot(tadpoles.lm, 1)\rIn this plot we are looking for an even “shotgun” like appearance in the residuals. We want an even dispersal around the grand mean. In this example, we have a spread of redisuals that does not appear to follow any non-linear trends.\rThere is no point trying to fit a straight line through data that is curved. If there is strong patterning in your residuals, try log-transforming your response or, fit a polynomial function (e.g. quadratic).\nClick the link below to see a nice interactive app that demonstrates what patterns of residuals you would expect with linear and curved relationships:\nLinear regression diagnostics\rhttps://gallery.shinyapps.io/slr_diag/\nTest your normality before moving on.\n\r"
},
{
	"uri": "/statistical-analysis/7_glms/",
	"title": "Generalised linear models (GLM&#39;s)",
	"tags": [],
	"description": "",
	"content": "So far, we have been using linear models which assume that our response variable is continuous. In earth and life sciences (ecology in particular) we are often working with discrete data, such as count data and binomial (presence/absence) data.\nThe linear models we have been using so far have been assuming a normal (or gaussian) distribution in our data. Generalised linear models (GLMs) allow us to fit alternative distributions to our data in order to more accurately analyse them.\nGLMs do make some important assumptions which we will need to check when we construct the model.\nOur binomial (logistic regression) does have some assumptions, but thankfully it is fairly resiliant and we dont need to test them. For any other distribution (poisson, gamma etc.) these are cruical.\nIt is important to note that part of fitting a GLM is using a link function. I won’t be explaining these in detail (yet), all you need to know is the default link method for binomial data is the logit() method. For more information see ?family.\nFor this analysis, we will be using the nestpredation.csv dataset\n str(nest) # view the structure\r## \u0026#39;data.frame\u0026#39;: 20 obs. of 2 variables:\r## $ shrubcover : int 16 20 11 15 19 31 5 12 9 10 ...\r## $ nestattacked: int 1 0 1 1 1 0 1 1 1 0 ...\r# This is okay. Our nest attacked column is an integer, but the glm will tell it to input as binomial so we dont need to change anything. nest.bin\u0026lt;-glm(nestattacked~shrubcover, data=nest, family=binomial)\rThe glm() commands follows the same structure as the lm() and aov() with the inclusion of the extra argument family. Family is where we specify our distribution. In this case, for our logistic regression, we specify a binomial distribution.\nOnce we have constructed our model, we can use the anova() command and the summary() commands to look at our results. The summary() commands p-values tend to be a little weird, so I prefer to use the anova() command to look at variable significance, and summary() to look at the model equation if I need it.\nanova(nest.bin, test=\u0026quot;Chisq\u0026quot;) # anova test using a chisq instead of F\r## Analysis of Deviance Table\r## ## Model: binomial, link: logit\r## ## Response: nestattacked\r## ## Terms added sequentially (first to last)\r## ## ## Df Deviance Resid. Df Resid. Dev Pr(\u0026gt;Chi) ## NULL 19 27.526 ## shrubcover 1 9.0911 18 18.434 0.002569 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThere is strong evidence that the probability of a nest being attacked varies with shrubcover (p\u0026lt;0.01).\nsummary(nest.bin)\r## ## Call:\r## glm(formula = nestattacked ~ shrubcover, family = binomial, data = nest)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8424 -0.5183 -0.2135 0.8024 1.5148 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 3.3782 1.6025 2.108 0.035 *\r## shrubcover -0.1883 0.0857 -2.198 0.028 *\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 27.526 on 19 degrees of freedom\r## Residual deviance: 18.434 on 18 degrees of freedom\r## AIC: 22.434\r## ## Number of Fisher Scoring iterations: 5\rLook at the differences in this table and the anova table. It’s hard to understand what is happening here and doesn’t provide you with the overall model effects, in most cases.\n"
},
{
	"uri": "/the-grammar-of-graphics/",
	"title": "The Grammar of Graphics",
	"tags": [],
	"description": "",
	"content": " The Grammar of Graphics Exploring the world of data visualisation using iterative plot building in ggplot2\n"
},
{
	"uri": "/the-grammar-of-graphics/1_ggplotgrammar/",
	"title": "The Grammar of ggplot2",
	"tags": [],
	"description": "",
	"content": "By now you should be fairly familiar with the R environment and decently familiar with tidyverse. You should be able to perform basic data manipulations, analyses and in general, understand the general concepts of working with data in R.\nTo me personally, data visualisation is the funnest part of data science. Being able to visually communicate your findings in new and interesting ways is exciting and a joy when you have so many ways to customise your message. Data analysis is important and useful, but the fun part is definately graphing!\nFor this module, we will be working soley within the ggplot graphing environment. Before we start, I should mention - R does have its own plotting functions which are powerful and very useful.\nGGPLOT is just better :)\nTo start, we will cover the bases of what ggplot is and how to build basic graphs with some free data built into R.\nResources\rHere are a few websites and useful places for ggplot graphing help. Its great to see examples of graphs along with code to help.\n\rGGPLOT CHEATSHEET - Seriously, this is amazing. There are a few of these on R studios’ website for a bunch of packages. I have a few of these printed on the wall of my office. Additonally, many of these can be accessed in the Help toolbar next to tools\rGGPLOT Reference Site - The official ggplot help site\n\rData Carpentry’s ggplot guide\n\rR Graphics Cookbook - Useful guides for graphing\r\rggplot is one of the many packages installed with tidyverse, but is also an important package on its own, that can be installed or loaded by itself using library(ggplot2).\nGGplot was built as a way to implement Leland Wilkinson’s “Grammar of Graphics”. The gammar of graphics broke up data visualisation into semantic components such as scales, layers and various aesthetic features. GGplot is a implementation of this scheme into the R environment and its crazy powerful.\nFirst, make sure ggplot2 or tidyverse is installed and loaded using the library() command.\nOnce we have that loaded into our environment, we need to create our first plot window following this basic structure.\nplot1 \u0026lt;- ggplot(data, aes(x = variable, y = variable)) +\rgeom_graph.type()\rplot1 # to view our object\rWe begin by creating a new object/variable of our choosing like almost everything else we do. We then use the ggplot() function to build a blank plot window.\nThe aes argument specifies what variables we want to plot in our blank window. aes stands for aesthetics, which is slightly confusing because it relates to what data we are displaying, not how we display it. It should be mentioned that the aesthetics and data can be specified on any of the geometric layers (“geoms”) and in some cases, you might have to.\nThe + geom_graph.type() will be the type of graph you want to display. The commonly used examples are:\n\rboxplot - + geom_boxplot()\rbarplot - + geom_bar()\rscatterplot - + geom_point()\r\rGeom stands for geometric, and tells R the type of geometric shape you want the data to form. You will need () closed brackets at the end of the geom_type() regardless of whether you choose to put anything inside them.\nThe next important thing is the use of additive building in ggplot. As you can see in the example, we use a + sign before adding the geom_type we want. Everything in ggplot uses these additive steps before each function. This allows you to add and change things on your graph step by step, building and viewing your graph as you go. This will make more sense as we go.\n\r"
},
{
	"uri": "/the-grammar-of-graphics/2_basicplots/",
	"title": "Basic plots",
	"tags": [],
	"description": "",
	"content": "To start, we will use the iris dataset that is built into tidyverse/ggplot2. To view the dataset, use the View() command like so:\nView(iris)\rOnce we have this, let’s setup a basic boxplot of some of the features of iris.\nThe iris dataset is built into tidyverse/ggplot2. The dataset is a pretty famous dataset by Edgar Anderson that gives the sepal length, width and petal length and width for three species of iris (n=50).\nWe are going to begin by plotting the sepal length for each species in a basic boxplot.\niris.box \u0026lt;- ggplot(iris, aes(x=Species,y=Sepal.Length)) +\rgeom_boxplot()\riris.box # We have to run a line with the name of the plot object to view the graph. \rSo far, pretty straight forward.\nYou will notice I saved the ggplot() graph to an object called iris.box. Because I saved the plot to an object, I have to run the object name to view the plot. This is identical to using the command print(iris.box).\nggplot graphs do no need to be saved as an object. You can run all of the commands singularly or as a group. The graph will still be produced. I personally prefer to save them to an object.\n Now let’s look at some others, such as a histogram.\niris.hist \u0026lt;- ggplot(iris, aes(x=Sepal.Length)) +\rgeom_histogram()\riris.hist\rThat’s pretty ugly, but a simple addition of binwidth=“value” will fix that. Binwidth refers to the width of each bin, or bar, in the frequency histogram. A bin width of 0.5 means each bar of the histogram will be equal to 0.5 on the x axis (e.g. 4, 4.5, 5, 5.5 etc).\niris.hist \u0026lt;- ggplot(iris, aes(x=Sepal.Length)) +\rgeom_histogram(binwidth = 0.5)\riris.hist\rNow let’s look at a scatterplot.\niris.scatter \u0026lt;- ggplot(iris, aes(x=Sepal.Length,y=Petal.Length)) + geom_point()\riris.scatter\rThe cool thing we can do with scatterplots is colour the points by a categorical feature such as Species. This is done by adding colour = “categorical variable name” in the aes brackets of the ggplot() command.\niris.scatter \u0026lt;- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, colour=Species)) +\rgeom_point()\riris.scatter\rMuch better. And it even adds a legend for us.\nNow we have this basic setup, we can start adding things to our graph. Due to the immense amount of customisations for our graphs, I will break these down in to sections as much as possible and explain as I go. We will work with the iris dataset for a while before moving to our analysed datasets.\n"
},
{
	"uri": "/the-grammar-of-graphics/themes/",
	"title": "Customising your graph",
	"tags": [],
	"description": "",
	"content": " So far, we have explore some basic plots and functions of ggplot. But, we have not explored how truly powerful and customisable ggplot is. This section is designed to show you a wealth of customisable options for the general appearance of your graph. Alot of this section is code-heavy and can get quite overwhelming but thankfully, alot of the code is repetitive. So remember, take a break, grab a coffee and proceed through the customisation section at your own pace. Alot of this isn\u0026rsquo;t critical to producing a simple graph and you can move on to the next section of plotting at any time.\nContent:  Themes  The easiest way to quickly modify your graph is to add one of the preset theme() commands. I will add each of them to the graph which will replace the previous theme. We can simply add items to our current graph object by adding the + sign. Keep in mind that if you dont “resave” it to the object, it wont stick around. If you want to keep a theme, either add it into the original ggplot command, or save it to the same or a new object.\n Axis lines  To change the axis lines and ticks (lines above each number on an axis) use the following. Theme argument\rDescription\raxis.line = element_line(insert changes here)\rThis will change both axes lines.\raxis.line.x = element_line(insert changes here)\rThis will change just the x axis.\raxis.line.y = element_line(insert changes here)\rThis will change just the y axis.\raxis.ticks = element_line(insert changes here)\rChange both axes ticks.\n Background  The plot and legend background colours can be changed using the following: Theme argument\rDescription\rpanel.background = element_rect(insert changes here)\rThis changes the background of the main plot itself. We need element_rect() as it is a rectangle geometric object.\rlegend.background = element_rect(insert changes here)\rThis will change the main area of the legend.\rlegend.key = element_rect(insert changes here)\rThis will change the small boxes that each of the factors levels are identified with.\n Grids  So far, our graph does not have the original ggplot grid lines because we removed them in our original graph. Before we start changing these, let’s save our beautiful masterpiece to an object/variable to simplify the theme() changing. iris.scatter \u0026lt;- iris.scatter + theme(panel.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;red\u0026quot;), legend.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;yellow\u0026quot;, size=1), legend.key = element_rect(fill = \u0026quot;gray50\u0026quot;, colour = \u0026quot;green\u0026quot;, size = 0.5)) +\rtheme(axis.line.x = element_line(colour = \u0026quot;skyblue\u0026quot;, size=2), axis.line.y = element_line(colour=\u0026quot;deeppink\u0026quot;, size = 2), axis.\n Axis labels  If we want to change the axis labels themselves, this is done using the labs() command. iris.scatter \u0026lt;- iris.scatter + labs(x = \u0026quot;Sepal Length (cm)\u0026quot;, y = \u0026quot;Petal Length (cm)\u0026quot;)\riris.scatter\rIf we wish to add a title to our plot (not overly common in publications) we can use the following. iris.scatter \u0026lt;- iris.scatter + labs(title= \u0026quot;Relationship between petal and sepal length\u0026quot;) iris.scatter\rAfter trying to use these labs() commands you will start to realise it hates anything slightly symbolic (subscript, superscript, degrees etc.\n Proper examples  ## Setting up the graph environment ##\riris.scatter.proper \u0026lt;- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, colour=Species, shape=Species)) + geom_point()\r## Making our theme ##\rplottheme \u0026lt;- theme(panel.background = element_rect(fill=\u0026quot;ghostwhite\u0026quot;),\rlegend.background = element_blank(),\rlegend.key = element_rect(fill=\u0026quot;ghostwhite\u0026quot;),\raxis.line = element_line(colour=\u0026quot;black\u0026quot;, size=1),\raxis.ticks = element_blank(),\raxis.title = element_text(colour=\u0026quot;royalblue3\u0026quot;, size=14),\rplot.title = element_text(face=\u0026quot;bold\u0026quot;, colour=\u0026quot;steelblue4\u0026quot;, size=16),\rlegend.title = element_text(colour=\u0026quot;royalblue3\u0026quot;, size=14),\rlegend.text = element_text(face=\u0026quot;italic\u0026quot;, colour=\u0026quot;steelblue4\u0026quot;, size=10),\raxis.text = element_text(colour=\u0026quot;steelblue4\u0026quot;, size=12),\rpanel.grid.major = element_line(colour=\u0026quot;gray80\u0026quot;),\rpanel.grid.minor = element_blank())\r## Applying the theme, adding some labels and changing some colours ##\riris.\n "
},
{
	"uri": "/the-grammar-of-graphics/themes/3_themes/",
	"title": "Themes",
	"tags": [],
	"description": "",
	"content": "The easiest way to quickly modify your graph is to add one of the preset theme() commands. I will add each of them to the graph which will replace the previous theme.\nWe can simply add items to our current graph object by adding the + sign. Keep in mind that if you dont “resave” it to the object, it wont stick around. If you want to keep a theme, either add it into the original ggplot command, or save it to the same or a new object.\niris.scatter + theme_bw()\riris.scatter + theme_classic()\riris.scatter + theme_dark() \riris.scatter + theme_gray() # The default ggplot theme\riris.scatter + theme_minimal()\riris.scatter + theme_light()\riris.scatter + theme_linedraw()\riris.scatter + theme_void()\rPretty significant changes to the graphs appearance with little effort.\nOf course, we can modify all the individual components of a theme without using one of the presets.\nThe best way to show this would be to look at the ?theme (help) window for this one. The general format for this is as follows.\niris.scatter \u0026lt;- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, colour=Species)) +\rgeom_point() +\rtheme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = \u0026quot;black\u0026quot;)) + theme(axis.text.x = element_text(colour = \u0026quot;black\u0026quot;, size = 12)) + theme(axis.text.y = element_text(colour = \u0026quot;black\u0026quot;, size = 12)) +\rtheme(plot.title = element_text(color=\u0026quot;blue\u0026quot;, size=12))\riris.scatter\rWithin the theme() command, we simply call the feature we want to change, followed by how we want to change it.\rFor the panel grids and background, we call element_blank() to make it blank. Changing that to element_line() for the grids, and element_rect() for the background would change them to lines and rectangle, respectively. From there we could pick colour, size etc.\nIn the axis.text lines, we are setting the text colour to “black” and the font size to 12.\nNow obviously, this is pretty daunting. But, you dont have to specify everything. You can very easily use one of the above preset themes (e.g. theme_minimal) and change one or two other things, such as axis line colour etc.\nTo save yourself writing all of the above theme() commands everytime you do a graph, you can save your favourite custom settings to its own object and add that to your graphs. Like so:\nsimpletheme \u0026lt;- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = \u0026quot;black\u0026quot;), axis.text.x = element_text(colour = \u0026quot;black\u0026quot;, size = 12), axis.text.y = element_text(colour = \u0026quot;black\u0026quot;, size = 12),plot.title = element_text(color=\u0026quot;blue\u0026quot;, size=12))\r# We simply direct all of our theme arguments to an object\riris.scatter \u0026lt;- iris.scatter + simpletheme # then, just add that object to our graph\rFor example, let’s add those custom theme settings to our boxplot we generated earlier.\niris.box + simpletheme\rOk, so that was alot of information that probably doesn’t make sense, so let’s break that down into its components.\n"
},
{
	"uri": "/the-grammar-of-graphics/themes/5_axislines/",
	"title": "Axis lines",
	"tags": [],
	"description": "",
	"content": "To change the axis lines and ticks (lines above each number on an axis) use the following.\n\r\r\r\rTheme argument\rDescription\r\r\r\raxis.line = element_line(insert changes here)\rThis will change both axes lines.\r\raxis.line.x = element_line(insert changes here)\rThis will change just the x axis.\r\raxis.line.y = element_line(insert changes here)\rThis will change just the y axis.\r\raxis.ticks = element_line(insert changes here)\rChange both axes ticks. Use the .x or .y to change just one axis at a time.\r\raxis.ticks.length = element_line(insert changes here)\rChange the length of the axes ticks.\r\raxis.text = element_text(insert changes here)\rChange the text on the axes TICKS. Use .x or .y to change just one.\r\raxis.title = element_text(insert changes here)\rChange the text on the axes LABELS/TITLES. Use .x or .y to change just one.\r\rplot.title = element_text(insert changes here)\rChange the plot title.\r\r\r\rJust use the colour and size arguments where appropriate. I am going to add these changes as a separate theme() command, but they can be added in the same command as last time.\niris.scatter + theme(panel.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;red\u0026quot;), legend.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;yellow\u0026quot;, size=1), legend.key = element_rect(fill = \u0026quot;gray50\u0026quot;, colour = \u0026quot;green\u0026quot;, size = 0.5)) +\rtheme(axis.line.x = element_line(colour = \u0026quot;skyblue\u0026quot;, size=2), axis.line.y = element_line(colour=\u0026quot;deeppink\u0026quot;, size = 2), axis.title.x = element_text(colour=\u0026quot;forestgreen\u0026quot;, size=14), axis.title.y = element_text(colour = \u0026quot;gold\u0026quot;, size=8), axis.ticks = element_blank())\rBeautiful, isn’t it?\nNow you may have notice the size command acts differently for line and text. For line it is based on a multiplier of the original. So a 2 will be two times its normal size. Element_text() has size as a font size. So 2 would be tiny and equivalent to 2pt font.\rAlternatively, you can use size = rel(number) to scale the text relative to base R’s plotting size.\n"
},
{
	"uri": "/the-grammar-of-graphics/themes/4_gridsbackground/",
	"title": "Background",
	"tags": [],
	"description": "",
	"content": "The plot and legend background colours can be changed using the following:\n\r\r\r\rTheme argument\rDescription\r\r\r\rpanel.background = element_rect(insert changes here)\rThis changes the background of the main plot itself. We need element_rect() as it is a rectangle geometric object.\r\rlegend.background = element_rect(insert changes here)\rThis will change the main area of the legend.\r\rlegend.key = element_rect(insert changes here)\rThis will change the small boxes that each of the factors levels are identified with.\r\r\r\rFor all arguments, you can replace the element_rect(), element_line() etc. with element_blank() to remove it.\nWithin each of the element_rect() we can change various things. The most common ones are:\n\r\rElement argument\rDescription\r\r\r\rfill = “colour”\rThis will change the overall colour of the object.\r\rcolour = “colour”\rThis will change the outline of the rectangle.\r\rsize = number\rThis will change the size/thickness of font and lines.\r\r\r\rEach of the “colour” arguments can be a specified a number of ways. The most common way is using one of the MANY predefined colours within R. A quick run down of these can be found here. For any of these, just put the name as it is spelt in that guide in quotations.\niris.scatter + theme(panel.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;red\u0026quot;), legend.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;yellow\u0026quot;, size=1), legend.key = element_rect(fill = \u0026quot;gray50\u0026quot;, colour = \u0026quot;green\u0026quot;, size = 0.5)) \r"
},
{
	"uri": "/the-grammar-of-graphics/themes/6_grids/",
	"title": "Grids",
	"tags": [],
	"description": "",
	"content": "So far, our graph does not have the original ggplot grid lines because we removed them in our original graph. Before we start changing these, let’s save our beautiful masterpiece to an object/variable to simplify the theme() changing.\niris.scatter \u0026lt;- iris.scatter + theme(panel.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;red\u0026quot;), legend.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;yellow\u0026quot;, size=1), legend.key = element_rect(fill = \u0026quot;gray50\u0026quot;, colour = \u0026quot;green\u0026quot;, size = 0.5)) +\rtheme(axis.line.x = element_line(colour = \u0026quot;skyblue\u0026quot;, size=2), axis.line.y = element_line(colour=\u0026quot;deeppink\u0026quot;, size = 2), axis.title.x = element_text(colour=\u0026quot;forestgreen\u0026quot;, size=14), axis.title.y = element_text(colour = \u0026quot;gold\u0026quot;, size=8), axis.ticks = element_blank())\rTo change the grid lines on your plot, it is done with the following:\n\r\r\r\rTheme argument\rDescription\r\r\r\rpanel.grid.major = element_line(insert changes here)\rChanges the major grid lines on the graph. Use .x or .y to change just one.\r\rpanel.grid.minor = element_line(insert changes here)\rChanges the minor grid lines on the graph. Use .x or .y to change just one.\r\r\r\rAgain, using the same principals of colour and size for these ones.\niris.scatter + theme(panel.grid.major = element_line(colour=\u0026quot;aquamarine\u0026quot;, size=1), panel.grid.minor = element_line(colour=\u0026quot;slategray2\u0026quot;, size=2)) \rJust like we did before, we can make all of these our own custom theme by directing them to an object.\nmasterpiece \u0026lt;- theme(panel.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;red\u0026quot;), legend.background = element_rect(fill=\u0026quot;lavender\u0026quot;, colour=\u0026quot;yellow\u0026quot;, size=1), legend.key = element_rect(fill = \u0026quot;gray50\u0026quot;, colour = \u0026quot;green\u0026quot;, size = 0.5), axis.line.x = element_line(colour = \u0026quot;skyblue\u0026quot;, size=2), axis.line.y = element_line(colour=\u0026quot;deeppink\u0026quot;, size = 2), axis.title.x = element_text(colour=\u0026quot;forestgreen\u0026quot;, size=14), axis.title.y = element_text(colour = \u0026quot;gold\u0026quot;, size=8), axis.ticks = element_blank(), panel.grid.major = element_line(colour=\u0026quot;aquamarine\u0026quot;, size=1), panel.grid.minor = element_line(colour=\u0026quot;slategray2\u0026quot;, size=2))\rNow let’s add that to our boxplot.\niris.box + masterpiece\rA true work of art!\n"
},
{
	"uri": "/the-grammar-of-graphics/themes/7_axislabels/",
	"title": "Axis labels",
	"tags": [],
	"description": "",
	"content": "If we want to change the axis labels themselves, this is done using the labs() command.\niris.scatter \u0026lt;- iris.scatter + labs(x = \u0026quot;Sepal Length (cm)\u0026quot;, y = \u0026quot;Petal Length (cm)\u0026quot;)\riris.scatter\rIf we wish to add a title to our plot (not overly common in publications) we can use the following.\niris.scatter \u0026lt;- iris.scatter + labs(title= \u0026quot;Relationship between petal and sepal length\u0026quot;) iris.scatter\rAfter trying to use these labs() commands you will start to realise it hates anything slightly symbolic (subscript, superscript, degrees etc.). To fix this is simple, but clumsy in how its executed. The following code uses the expression() argument to solve these issues.\nI have written a x-axis label that does not make sense, in an effort to display the most common issues. These are a few of my own, so they do not make any sense with the given graph.\niris.scatter \u0026lt;- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, colour=Species)) +\rgeom_point() +\rlabs(x = expression(Sepal~Length[cm]), y = expression(Petal~Length^cm))+\rlabs(title = expression(Sepal~by~Petal~at~\u0026quot;20\u0026quot;*degree*C))\riris.scatter\rThis example, while nonsensical, demonstrates some of the major quirks with the expression() argument/command.\nAcross all of the expression arguments, we specify a space between characters/words by using a tilde ~. In our x axis, we specify a subscript (lower) by using square brackets []. Anything inside these will be placed below the preceeding character. Similarly, we specify superscript by using the caret ^ to denote power. Anything placed after will be placed above the preceeding character.\nIn the title line, (note that I had to place the title on a separate line…ggplot is precious sometimes) we see quotations around the 20. This is because expression does not appreciate anything starting with a number. The next thing is the use of both “degree” and the asterix __*__. The asterix is used when we need to write something like “degree” or “pi” to specify a symbol, but when we want it to be next to something, like a C for degree*C.\nI hope this helps understand the clumsy execution of complex axis labels.\n"
},
{
	"uri": "/the-grammar-of-graphics/themes/8_properexamples/",
	"title": "Proper examples",
	"tags": [],
	"description": "",
	"content": "## Setting up the graph environment ##\riris.scatter.proper \u0026lt;- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, colour=Species, shape=Species)) + geom_point()\r## Making our theme ##\rplottheme \u0026lt;- theme(panel.background = element_rect(fill=\u0026quot;ghostwhite\u0026quot;),\rlegend.background = element_blank(),\rlegend.key = element_rect(fill=\u0026quot;ghostwhite\u0026quot;),\raxis.line = element_line(colour=\u0026quot;black\u0026quot;, size=1),\raxis.ticks = element_blank(),\raxis.title = element_text(colour=\u0026quot;royalblue3\u0026quot;, size=14),\rplot.title = element_text(face=\u0026quot;bold\u0026quot;, colour=\u0026quot;steelblue4\u0026quot;, size=16),\rlegend.title = element_text(colour=\u0026quot;royalblue3\u0026quot;, size=14),\rlegend.text = element_text(face=\u0026quot;italic\u0026quot;, colour=\u0026quot;steelblue4\u0026quot;, size=10),\raxis.text = element_text(colour=\u0026quot;steelblue4\u0026quot;, size=12),\rpanel.grid.major = element_line(colour=\u0026quot;gray80\u0026quot;),\rpanel.grid.minor = element_blank())\r## Applying the theme, adding some labels and changing some colours ##\riris.scatter.proper \u0026lt;- iris.scatter.proper + plottheme +\rlabs(x=\u0026quot;Sepal Length (cm)\u0026quot;, y=\u0026quot;Petal Length (cm)\u0026quot;, title=\u0026quot;Relationship between Sepal Length and Petal Length\u0026quot;) +\rscale_colour_manual(values = c(\u0026quot;mediumorchid1\u0026quot;, \u0026quot;mediumorchid3\u0026quot;, \u0026quot;mediumorchid4\u0026quot;))\r## Displaying our graph ##\riris.scatter.proper\rPretty cool example of changing things around for the “better”. You might notice a few extra things I have changed in this graph.\nIn the aes() section at the start, I introduced the shape command which changes the shape for each level of a factor. Doing this alongside colour= allows us to change the colour and symbol of the points themselves.\nFurther down, I then changed the colour of the points using scale_colour_manual() and adding the colour values for the levels in order. There are many different ways you can do this, but I find this works the best. There are scale_manual commands for fill, group, shape etc.\nIn the theme() section, I covered most things we have done so far but added an additional argument to legend.text and plot.title. This is the face argument which allows us to add italics, bold or others to our text.\nI will add more to this page as I produce and discover cool graphs\r\r"
},
{
	"uri": "/the-grammar-of-graphics/bar-plots/",
	"title": "Bar plots",
	"tags": [],
	"description": "",
	"content": " This section is devoted to all things bar graphing. This is mostly aimed at graphing basic ANOVA results, displaying errorbars, statistical significance through a Tukey\u0026rsquo;s HSD and the difference between \u0026ldquo;stacked\u0026rdquo; and \u0026ldquo;dodged\u0026rdquo; plots.\nContent:  Basic bar plots  For this section, we will be using the weeds dataset where we performed a two-factor ANOVA For a quick reminder: weeds.aov2 \u0026lt;- aov(flowers ~ species * soil, data = weeds)\ranova(weeds.aov2)\r## Analysis of Variance Table\r## ## Response: flowers\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2368.6 1184.31 9.1016 0.0005203 ***\r## soil 1 238.5 238.52 1.8331 0.1830080 ## species:soil 2 155.0 77.52 0.\n Bar graphs - part 2  In the last example, we plotted a single column graph. To plot multiple columns, for example a soil by species interaction, is quite simple. Firstly, we will run our summarise command, adding the soil column into our group_by() command to generate the means and standard error for the soil, species combinations. weeds.summarise2 \u0026lt;- weeds %\u0026gt;% group_by(species, soil) %\u0026gt;%\rsummarise(mean = mean(flowers), se=sd(flowers/sqrt(n())))\rWe plot multiple columns by specifying one column in our x axis, and filling/colouring by another.\n Errorbars  Error bars are a simply addition to your graph, utilising their own geometric command geom_errorbar().\rTo add the error bars, we use the following command ggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se))\rThis is suprisingly simple. All we do is specify the aesthetic (aes) where we compute our minimum and maximum y values for our bars as our mean column +/- our standard error column. We can further customise our errorbars through the use of a few arguments.\n Significant notation  When presenting our results to an audience (paper or presentation) it is important to communicate our results clearly in a manner that is understandable to a wider audience. Tha main way to do so with an Analysis of Variance, is using a post-hoc test like a Tukeys Honest Significant Difference (Tukeys HSD). This will analyse the differences between the levels within a factor to distinguish which levels are significantly different from one another.\n Finalising our Barplot  Thats the general process for setting up a column graph for ANOVA data. It can take some time, but we get alot of freedom in how we present this. Let’s spruce up our graph to a finalised form, before we save it to an image file. weeds.bar \u0026lt;- ggplot(weeds.summarise, aes(x=species, y=mean, fill=species))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;, show.legend=F, colour=\u0026quot;black\u0026quot;)+\rlabs(x=\u0026quot;Weed Species\u0026quot;, y= expression(Flowers~(m^3)))+\rtheme(panel.background = element_blank(), panel.grid = element_blank(), axis.line = element_line(colour = \u0026quot;black\u0026quot;, size=1), axis.\n "
},
{
	"uri": "/the-grammar-of-graphics/bar-plots/1_barplots/",
	"title": "Basic bar plots",
	"tags": [],
	"description": "",
	"content": "For this section, we will be using the weeds dataset where we performed a two-factor ANOVA\n For a quick reminder:\nweeds.aov2 \u0026lt;- aov(flowers ~ species * soil, data = weeds)\ranova(weeds.aov2)\r## Analysis of Variance Table\r## ## Response: flowers\r## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## species 2 2368.6 1184.31 9.1016 0.0005203 ***\r## soil 1 238.5 238.52 1.8331 0.1830080 ## species:soil 2 155.0 77.52 0.5958 0.5557366 ## Residuals 42 5465.1 130.12 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rFrom this, only Species was significant. For this dataset with a continuous Y and categorical X we would plot a bargraph.\nThere are three main ways to display a bar/column graph, geom_col(), geom_bar() and stat_summary(). I will cover each of them in some depth, showing the benefits to each. Here is a quick breakdown to begin.\n\r\r\r\rPlot\rPro\rCon\r\r\r\rgeom_col()\rSimple and effective, defaults to displaying data as is\rErrorbars are finicky\r\rgeom_bar()\rErrorbars work well, displays sample size/counts by default\rRequires a single argument to match geom_col\r\rstat_summary()\rQuick calculation of mean, used across all geometric types\rDifficult to code and errorbars just flat out dont work\r\r\r\rI find best way to generate the bargraph properly, is to use the summarise() command to generate our means and standard errors before plotting. This extra step saves alot of hassle and you can copy this code across any dataset, changing the column names. We can generate these within ggplot, but it leads to complications (see stat_summary() below).\nweeds.summarise \u0026lt;- weeds %\u0026gt;% group_by(species) %\u0026gt;%\rsummarise(mean = mean(flowers), se=sd(flowers)/sqrt(n()))\rThis is a quick way to generate our mean and se for flowers for each species. Now, we can graph our results in a bargraph.\nggplot(weeds.summarise, aes(x=species, y=mean, fill=species)) +\rgeom_col()\rThis will generate a pretty basic graph. You will notice that I used fill instead of colour. If you use colour on a column/bar graph it will colour the outline. Using fill will fill the entire bar according to the species.\nWe used geom_col() to generate a column graph. You can use geom_bar() but it requires a stat = argument. If you use geom_bar(), stat = “identity” use the numbers in the mean column of our data, displaying data as it is in the data frame, rather than counting the number of cases in each X position (its default state).\nI personally use geom_bar() as I find it easier to do errorbars later. Future pages use geom_bar()\nggplot(weeds.summarise, aes(x=species, y=mean, fill=species)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)\rRegardless of what way you graph this, they look the same. For now, let’s work with geom_bar(). Let’s fix up the graph as much as we want, until we are happy.\nweeds.bar \u0026lt;- ggplot(weeds.summarise, aes(x=species, y=mean, fill=species))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;, show.legend=F, colour=\u0026quot;black\u0026quot;)+\rlabs(x=\u0026quot;Weed Species\u0026quot;, y= expression(Flowers~(m^3)))+\rtheme(panel.background = element_blank(), panel.grid = element_blank(), axis.line = element_line(colour = \u0026quot;black\u0026quot;, size=1), axis.text = element_text(colour=\u0026quot;lightsteelblue4\u0026quot;, size=12), axis.title = element_text(colour=\u0026quot;steelblue\u0026quot;, size=14, face=\u0026quot;bold\u0026quot;))+\rscale_fill_manual(values = c(\u0026quot;lightblue\u0026quot;, \u0026quot;steelblue\u0026quot;, \u0026quot;darkslateblue\u0026quot;))\rweeds.bar\rSo, now we have our graph in a “nicer” format, we can see that there are some cruical points of information missing from this graph. Most notably, the errorbars and letters or some other notation that denotes statistical differences between the levels (i.e. Tukeys HSD results).\nTo remove the legend like I have, include the show.legend argument in your geom_bar() command and set it to false. e.g. geom_bar(stat=\u0026quot;identity\u0026quot;, show.legend=F)\n "
},
{
	"uri": "/the-grammar-of-graphics/bar-plots/2_barplot2/",
	"title": "Bar graphs - part 2",
	"tags": [],
	"description": "",
	"content": "In the last example, we plotted a single column graph. To plot multiple columns, for example a soil by species interaction, is quite simple.\nFirstly, we will run our summarise command, adding the soil column into our group_by() command to generate the means and standard error for the soil, species combinations.\nweeds.summarise2 \u0026lt;- weeds %\u0026gt;% group_by(species, soil) %\u0026gt;%\rsummarise(mean = mean(flowers), se=sd(flowers/sqrt(n())))\rWe plot multiple columns by specifying one column in our x axis, and filling/colouring by another. ggplot also plots our legend automatically, which is handy.\nggplot(weeds.summarise2, aes(x=soil, y=mean, fill=species)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)\rHowever, you can see the bar graph has stacked the species ontop of one another. To fix this, include the position=\u0026quot;dodge\u0026quot; argument in your geom_bar(), like so.\nggplot(weeds.summarise2, aes(x=soil, y=mean, fill=species)) + geom_bar(stat=\u0026quot;identity\u0026quot;, position=\u0026quot;dodge\u0026quot;)\rHorizontal bar graphs\rSometimes, a vertical bargraph just doesn’t cut it.\nggplot(weeds.summarise2, aes(x=soil, y=mean, fill=species)) + geom_bar(stat=\u0026quot;identity\u0026quot;, position=\u0026quot;dodge\u0026quot;)+\rcoord_flip()\rBy using coord_flip() we can rotate the entire graph into its side, displaying our bar graph horizontally instead of vertically. This will work for pretty much every ggplot graph.\nNow that we have covered the basics of plotting bar/column graphs we can see that there are some cruical points of information missing from these graphs. Most notably, the errorbars and letters or some other notation that denotes statistical differences between the levels (i.e. Tukeys HSD results).\n\r"
},
{
	"uri": "/the-grammar-of-graphics/bar-plots/3_errorbars/",
	"title": "Errorbars",
	"tags": [],
	"description": "",
	"content": "Error bars are a simply addition to your graph, utilising their own geometric command geom_errorbar().\rTo add the error bars, we use the following command\nggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se))\rThis is suprisingly simple. All we do is specify the aesthetic (aes) where we compute our minimum and maximum y values for our bars as our mean column +/- our standard error column.\nWe can further customise our errorbars through the use of a few arguments. Lets explore those iteratively.\nSize\rggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), size = 2)\rThe size argument increases the thickness of the errorbars\n\rColour, linetype and transparency\rWe can also change the colour, linetype, and transparency.\nggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), colour = \u0026quot;red\u0026quot;)\rColour is as straightforward as usual, just name a colour.\nFor linetype, we specify a number between 1-6 that corresponds to R’s built in linetypes.\rggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), colour = \u0026quot;red\u0026quot;, linetype = 2)\rTransparency is specified throught the alpha argument, giving a number between 0 (transparent) and 1 (solid). It’s pretty pointless for errorbars, but it can be used for many other functions.\nggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), size = 2, alpha = 0.5)\r\rWidth\rThe width argument is arguably the most important aesthetical customisation for errorbars. Width customises the width of the errorbars compared to the width of the bars.\nggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = 0.5)\rThe default width value for errorbars is 0.9, that is 90% of the width of the bar.\n\rPosition\rWhen plotting multiple errorbars, much like with standard bars, the default structure is to “stack” the bars in a single column. We can alter the position of errorbars through the use of the “position” argument.\nAltering the position of errorbars in bar graphs has given me alot of headaches over the years. This gets more finicky the more complex your graph is, so I hope the below solution fixes all of your future problems :)\nFor this example, I am using our “interaction” bargraph to demonstrate.\nggplot(weeds.summarise2, aes(x=soil, y=mean, fill=species)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;, position=\u0026quot;dodge\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = 0.5, position=position_dodge(0.9))\rTo alter the position of our errorbars we include the position=position_dodge(0.9) argument to match our original position=\u0026quot;dodge\u0026quot; in our bar line. You will notice these two arguments have different values and syntax. The standard position=\u0026quot;dodge\u0026quot; does work for the errorbars, but I have had very mixed results. The position=position_dodge(0.9) is slightly more annoying, but tends to work alot more. The 0.9 value is the default for the errorbars and refers to the distance between the middle errorbar and the left and/or right errobars when dodged. If your errobar comes out a little “funky”, modify this value.\n\r"
},
{
	"uri": "/the-grammar-of-graphics/bar-plots/4_tukeys/",
	"title": "Significant notation",
	"tags": [],
	"description": "",
	"content": "When presenting our results to an audience (paper or presentation) it is important to communicate our results clearly in a manner that is understandable to a wider audience. Tha main way to do so with an Analysis of Variance, is using a post-hoc test like a Tukeys Honest Significant Difference (Tukeys HSD). This will analyse the differences between the levels within a factor to distinguish which levels are significantly different from one another.\nTo jog our memory from our test, let’s run the Tukeys test from our analysis module using the HSD.test() from the agricolae package.\nlibrary(agricolae)\rHSD.test(weeds.aov2, \u0026quot;species\u0026quot;, console=TRUE)\r## ## Study: weeds.aov2 ~ \u0026quot;species\u0026quot;\r## ## HSD Test for flowers ## ## Mean Square Error: 130.122 ## ## species, means\r## ## flowers std r Min Max\r## Coprosma 24.1250 11.13478 16 13 52\r## Olearia 36.7500 12.08580 16 16 55\r## Pultenaea 40.5625 10.97858 16 20 57\r## ## Alpha: 0.05 ; DF Error: 42 ## Critical Value of Studentized Range: 3.435823 ## ## Minimun Significant Difference: 9.798198 ## ## Treatments with the same letter are not significantly different.\r## ## flowers groups\r## Pultenaea 40.5625 a\r## Olearia 36.7500 a\r## Coprosma 24.1250 b\rAccording to the tukeys results, Coprosma is significantly different from the others. So we will label it A and the others B.\nThere are two main ways to plot notation on a graph, a manual way using coordinates, and an automatic way. We will cover the manual way first so we can see how it works before preceeding to the easy method.\nggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = 0.5)+\rgeom_text(label = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;), aes(y = c(28.5, 41, 44.5), x = species), size = 6)\r# try including the geom_text() in your original weeds.bar code. \rAdding notation is done through geom_text(). We need to specify the labels (in order from left -\u0026gt; right) along with the aesthetic coordinates on the x and y axis. The X axis we can direct it to our original x axis data (species) and it will sit in the centre of the column. The Y coordinates are the location on the Y axis the text should sit.\nThis method is very finicky but is a great method if you are looking to plot one letter/symbol on the graph. You can add multiple geom_text() commands if needed.\nThe quicker solution to this, is to use a combination of our errorbars and an additional argument called vjust (vertical adjustment).\nggplot(weeds.summarise, aes(x=species, y=mean)) +\rgeom_bar(stat=\u0026quot;identity\u0026quot;)+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), width = 0.5)+\rgeom_text(label = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;), aes(y = mean+se, x = species),vjust = -0.5, size = 6)+ ylim(0, 50)\rWe simply specify our Y coordinates as the top of our error bar (mean + se) and use the vjust (vertical ajustment) argument to move it slightly above the bar. You might have to change your ylim to display the last letter, which got cut off.\n"
},
{
	"uri": "/the-grammar-of-graphics/bar-plots/5_finalising/",
	"title": "Finalising our Barplot",
	"tags": [],
	"description": "",
	"content": "Thats the general process for setting up a column graph for ANOVA data. It can take some time, but we get alot of freedom in how we present this.\nLet’s spruce up our graph to a finalised form, before we save it to an image file.\nweeds.bar \u0026lt;- ggplot(weeds.summarise, aes(x=species, y=mean, fill=species))+\rgeom_bar(stat=\u0026quot;identity\u0026quot;, show.legend=F, colour=\u0026quot;black\u0026quot;)+\rlabs(x=\u0026quot;Weed Species\u0026quot;, y= expression(Flowers~(m^3)))+\rtheme(panel.background = element_blank(), panel.grid = element_blank(), axis.line = element_line(colour = \u0026quot;black\u0026quot;, size=1), axis.text = element_text(colour=\u0026quot;lightsteelblue4\u0026quot;, size=12), axis.title = element_text(colour=\u0026quot;steelblue\u0026quot;, size=14, face=\u0026quot;bold\u0026quot;))+\rscale_fill_manual(values = c(\u0026quot;lightblue\u0026quot;, \u0026quot;steelblue\u0026quot;, \u0026quot;darkslateblue\u0026quot;))+\rgeom_errorbar(aes(ymin = mean-se, ymax = mean+se), width=0.5)+\rgeom_text(label = c(\u0026quot;A\u0026quot;, \u0026quot;B\u0026quot;, \u0026quot;B\u0026quot;), aes(y=mean+se, x=species), vjust=-0.5, size=6) +\rylim(0, 50)\rweeds.bar\rOnce we are satisfied with our final product, we can save it as a image file to our current working directory. Simply plot the graph again, by calling the object name, then use the ggsave() command like so.\nweeds.bar #producing the graph again\rggsave(\u0026quot;weeds_bargraph.png\u0026quot;) # specify the name and filetype (.jpeg, .png, .tif etc.). You can also specify the width and heigh of your final image\rggsave() will save the last plot you produced into your current working directory. You need to specify the name (in my case “weeds_bargraph”) and the filetype (.jpeg in my example). By default, it should save a 7 cm x 7cm image. If you want to change that, use the width = or height = arguments, like so. For higher resolution images, try .tif\nggsave(\u0026quot;weeds_bargraph.jpeg\u0026quot;, width=9, height=7)\rI wanted a slightly wider figure but it’s personal preference.\rFor very large or faceted graphs, you will need to change the width and height.\nAnd there we have it! We have produced and saved our own graph. This may have seemed daughting or a long process, but it’s very methodical once you get used to it. For an easier time, just use one of the preset theme commands like theme_minimal() to do all the aesthetical work for you :)\n"
},
{
	"uri": "/the-grammar-of-graphics/scatter-plots/",
	"title": "Scatter plots and Lines",
	"tags": [],
	"description": "",
	"content": " In this section, we will cover everything to do with scatterplots. The main focus of this section is plotting the results of a linear regression and as such most of this will be aimed at lines. In the future, line graphs and scatterplots will be separated to their own \u0026ldquo;modules\u0026rdquo;.\nContent:  Scatter plots  For this section, we will be using the tadpoles.csv data set The second dataset we analysed tadpole abundance in different sized ponds using a linear model/regression. Plotting linear regressions is really straightforward, but can be done a couple of different ways, depending on what you wish to accomplish. First, let’s run the basic analysis again (excluding the reeds factor). tadpoles.lm \u0026lt;- lm(abundance ~ pondsize, data = tadpoles)\rsummary(tadpoles.lm)\r## ## Call:\r## lm(formula = abundance ~ pondsize, data = tadpoles)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -73.\n Linear Lines  To produce a line on our graph, the easiest solution is using geom_smooth(method=lm). geom_smooth() by default will produce a loess smooth through our graph with confidence intervals. Since we have run a linear model, we specify the method of the geometric shape to fit that of a linear model (lm). ggplot(tadpoles, aes(x=pondsize, y=abundance)) +\rgeom_point(alpha = 0.5)+\rgeom_smooth(method=lm)\rmethod=lm tells the smooth line to plot a linear relationship between the variables in the graph environment.\n Logistic regression  For this section, we will be using the nestpredation.csv data set In our third dataset, we analysed the nest predation dataset using a generalised linear model with a binomial distribution, also known as a Logistic Regression. In this scenario, our data is measuring whether a nest was attacked or not in areas of different shrubcover. When we analyse this using a GLM, it is calculating the probability of a nest being attacked, given different values of shrubcover.\n "
},
{
	"uri": "/the-grammar-of-graphics/scatter-plots/1_scatterplot/",
	"title": "Scatter plots",
	"tags": [],
	"description": "",
	"content": "For this section, we will be using the tadpoles.csv data set\n The second dataset we analysed tadpole abundance in different sized ponds using a linear model/regression. Plotting linear regressions is really straightforward, but can be done a couple of different ways, depending on what you wish to accomplish.\nFirst, let’s run the basic analysis again (excluding the reeds factor).\ntadpoles.lm \u0026lt;- lm(abundance ~ pondsize, data = tadpoles)\rsummary(tadpoles.lm)\r## ## Call:\r## lm(formula = abundance ~ pondsize, data = tadpoles)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -73.546 -29.752 -8.026 37.978 77.652 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 23.8251 25.8455 0.922 0.36662 ## pondsize 1.7261 0.5182 3.331 0.00303 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 49.42 on 22 degrees of freedom\r## Multiple R-squared: 0.3352, Adjusted R-squared: 0.305 ## F-statistic: 11.09 on 1 and 22 DF, p-value: 0.003032\rFor this, we will be setting up a scatter plot (geom_point) of our points and then adding the line separately.\nggplot(tadpoles, aes(x=pondsize, y=abundance)) +\rgeom_point()\rThats our basic scatter plot. Simply using the geom_point() we covered breifly in the basic plots section.\nFrom here, we can customise our points using a variety of arguments within geom_point().\nColour\rColouring our points can be done in two ways. We can use the colour commands within our aesthetics and colour by a factor in our dataset, or, we can colour all the points within geom_point(). Lets cover the aes() commands.\nggplot(tadpoles, aes(x=pondsize, y=abundance, colour=reeds, shape = reeds)) +\rgeom_point()\rAs we covered in the basic plotting section, changing the colour and shape of points can be done through the use of “colour” and “shape” arguments within the aesthetics of ggplot or any geometric object (e.g. geom_point). To change the colour of these manually simply use scale_colour_manual() or scale_shape_manual() like so:\nggplot(tadpoles, aes(x=pondsize, y=abundance, colour=reeds, shape = reeds)) +\rgeom_point()+\rscale_colour_manual(values = c(\u0026quot;mediumspringgreen\u0026quot;, \u0026quot;forestgreen\u0026quot;, \u0026quot;black\u0026quot;))+\rscale_shape_manual(values = c(15, 16, 17))\rEach of the scale commands requires you to list the colours/shapes within a concatenated (c) list. This will be all most of you will ever need so simply copy those lines and replace/add values as you need.\n\rColours can be found here\n\rShapes can be found here\n\r\r\r"
},
{
	"uri": "/the-grammar-of-graphics/scatter-plots/2_plottinglines/",
	"title": "Linear Lines",
	"tags": [],
	"description": "",
	"content": "To produce a line on our graph, the easiest solution is using geom_smooth(method=lm). geom_smooth() by default will produce a loess smooth through our graph with confidence intervals. Since we have run a linear model, we specify the method of the geometric shape to fit that of a linear model (lm).\nggplot(tadpoles, aes(x=pondsize, y=abundance)) +\rgeom_point(alpha = 0.5)+\rgeom_smooth(method=lm)\rmethod=lm tells the smooth line to plot a linear relationship between the variables in the graph environment. You will see it automatically plots confidence intervals and colours the line blue. By default it will only extend to the range of our data, which is good. Both of these can be changed with the following:\nggplot(tadpoles, aes(x=pondsize, y=abundance)) +\rgeom_point(alpha = 0.5)+\rgeom_smooth(method=lm, se = FALSE, fullrange = TRUE)\rse = FALSE will turn off the standard error/confidence intervals for the line. This is set to true by default.\nfullrange = TRUE will extrapolate the line to the fullrange of the x \u0026amp; y axes. This should only be used it you are confident in what you are doing, as it does extrapolate data outside of what you collected.\nColour and line type\rColouring the the line follows the same principles as points and bars. colour = will colour the line itself, while fill = will colour the ribbon/confidence intervals.\nggplot(tadpoles, aes(x=pondsize, y=abundance)) +\rgeom_point(alpha = 0.5)+\rgeom_smooth(method=lm, colour = \u0026quot;red\u0026quot;, fill = \u0026quot;mediumpurple1\u0026quot;)\rWe can also change the line type using linetype = and specifying one of the 6 line types.\nggplot(tadpoles, aes(x=pondsize, y=abundance)) +\rgeom_point(alpha = 0.5)+\rgeom_smooth(method=lm, colour = \u0026quot;red\u0026quot;, linetype = 2)\r\rMultiple Lines\rWe can also plot multiple lines using the colour argument within the aesthetics (aes) of our graph and colour by a factor.\nggplot(tadpoles, aes(x=pondsize, y=abundance, colour = reeds)) +\rgeom_point(alpha = 0.5)+\rgeom_smooth(method=lm) \rThis time, we plot three lines by using the reeds factor. You can change the colouring and shape of each of the lines using the same commands as with points and bars.\nYou dont need to specify the aesthetic variables in the ggplot command, you can do so in each separate geom line by using aes(). This means you can produce three lines using colour in geom_smooth, but keep the points normal in geom_point. This is the same across all ggplot graphs\n This is an example of that point.\nggplot(tadpoles) +\rgeom_point(aes(x=pondsize, y=abundance))+\rgeom_smooth(aes(x=pondsize, y=abundance, colour = reeds), method=lm) \rYou can see in this example, that the points are black, while the three lines are coloured by reeds.\n\r"
},
{
	"uri": "/the-grammar-of-graphics/scatter-plots/3_logisticregression/",
	"title": "Logistic regression",
	"tags": [],
	"description": "",
	"content": "For this section, we will be using the nestpredation.csv data set\n In our third dataset, we analysed the nest predation dataset using a generalised linear model with a binomial distribution, also known as a Logistic Regression.\nIn this scenario, our data is measuring whether a nest was attacked or not in areas of different shrubcover. When we analyse this using a GLM, it is calculating the probability of a nest being attacked, given different values of shrubcover. As such, we need to plot this in a similar manner.\nFirst let’s demonstrate what happens when we don’t take the binomial distribution into account.\nggplot(nest, aes(x=shrubcover, y=nestattacked)) + geom_point()\rNotice how it has plotted the points at either 0 or 1 for each of the corresponding shrubcover values. This does not tell us anything about the likelihood of a nest being attacked given a value of shrubcover.\nThere are multiple methods for producing this plot. The one we will be using generates the relationship between our variables in the code itself.\nggplot(nest,aes(x=shrubcover, y=nestattacked)) +\rgeom_smooth(method = glm, method.args= list(family=\u0026quot;binomial\u0026quot;))\rThis method utilises the geom_smooth() function we were using for our linear model. This time we specify the glm relationship in the method argument, instead of lm. We also need to include a second argument called method.args which stands for method arguments, or, additional arguments for the method we have specified. We need to include this so we can inform our code that our distribution (family) is binomial. By including this, we produce our probability curve\nAs before, we can edit all of the things we did with the linear line because we are using the same command geom_smooth(). Like removing our errorbars.\nggplot(nest,aes(x=shrubcover, y=nestattacked)) +\rgeom_point()+\rgeom_smooth(method = glm, method.args= list(family=\u0026quot;binomial\u0026quot;), se=FALSE)\r"
},
{
	"uri": "/datasets/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "Example datasets provided courtesy of Professor Kristine French\n  Data Sets   frog_environmental.csv  (2 ko)   frogs.csv  (1 ko)   insecticide.csv  (0 ko)   nestpredation.csv  (0 ko)   site.csv  (0 ko)   tadpoles.csv  (0 ko)   weeds.csv  (1 ko)    "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/",
	"title": "Homepage",
	"tags": [],
	"description": "",
	"content": " Staring At R This site is a series of self-guided tutorials on using the R statistical package for data manipulation, analysis and visualisation including the following:\n Downloading and installing R \u0026amp; Rstudio\n Setting up your file system and working environment in a reproducable manner\n Manipulating your data in a reproducable manner\n Statistical analysis, covering analysis of variance, linear regressions and generalised linear models (more to come!)\n Visualisation of your results in a clear and customisable manner\n  The initial focus of this site is data science techniques for ecology based research, however, this will hopefully improve in scope in the future. Tutorials have initially been developed as part of a teaching module for undergraduate students in the School of Biology at the University of Wollongong but have been expanded beyond that initial development.\nSpecial thanks to Associate Professor Alistair Poore \u0026amp; Associate Professor Will Cornwell for their advice and excellent work on Environmental Computing, which was a huge inspiration and resource during the development of this site.\nSite administrator: Mitchell Stares\nDisclaimer: This site has been developed soley by Mitchell Stares, a PhD candidate in forest ecology at the University of Wollongong. Material for this site has been partly developed as material for an undergraduate subject at the University of Wollongong, while under employment as an associate lecturer. Mitchell holds no formal qualifications or certifications in statisitics, mathematics or computer science. Knowledge has stemmed from years of coding in R, self-taught through tutorials, textbooks and colleagues. Many of the resources for this site have been adapted from multiple online websites, and references to each of those websites will be updated shortly.\nLast updated: 14th November, 2018\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]